{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/chuuhtetnaing/myanmar-language-dataset-collection/refs/heads/main/A%20Corpus%20of%20Modern%20Burmese/allfiles.txt\"\n",
        "output_path = \"allfiles.txt\"\n",
        "\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Raise error if download failed\n",
        "\n",
        "with open(output_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"Downloaded {output_path} successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq8UBeLZz8kg",
        "outputId": "c9ac216f-7288-447e-d383-e33d5207de41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded allfiles.txt successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "size_mb = os.path.getsize(\"allfiles.txt\") / (1024 * 1024)\n",
        "print(f\"File size: {size_mb:.2f} MB\")\n",
        "##data Loading Stage Done Now Clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVwb7PIk0CEo",
        "outputId": "521d6ac3-5f98-4179-e734-c6fad3bba933"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 8.40 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CLEan Done\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove extra whitespace and control characters\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "cleaned_lines = []\n",
        "with open(\"allfiles.txt\", \"r\", encoding=\"utf-8\") as fin:\n",
        "    for line in fin:\n",
        "        cleaned = clean_text(line)\n",
        "        if cleaned:\n",
        "            cleaned_lines.append(cleaned)\n",
        "\n",
        "# Save cleaned file\n",
        "with open(\"allfiles_cleaned.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
        "    for line in cleaned_lines:\n",
        "        fout.write(line + \"\\n\")\n",
        "\n",
        "print(f\"Cleaning done! Saved {len(cleaned_lines)} lines to allfiles_cleaned.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjTYVZQ20Szo",
        "outputId": "79d4ddf8-ac68-44e9-f1b0-18b4613089eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning done! Saved 18061 lines to allfiles_cleaned.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"allfiles_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(10):\n",
        "        print(f.readline())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik5IwODk0dAv",
        "outputId": "2d4bbab6-0e53-4257-a87a-0bc74d9a1b9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿(၁) လေးစားရပါသော သဘာပတိကြီးနှင့် ကြွရောက်လာတဲ့ ပရိသတ်များရှင့်။ ဒီကနေ့ကတော့ အဓိကအားဖြင့် လွတ်လပ် ရေး မရမီ မြန်မာနိုင်ငံမှာ လုပ်ဆောင်ခဲ့တဲ့ ကျောင်းသားကြီးများ၊ အမျိုးသားရေးကို လုပ်ခဲ့တဲ့ ပုဂ္ဂိုလ်ကြီးများရဲ့ ကဏ္ဍကို ဖော်ပြတဲ့နေ့ ဖြစ်ပါတယ်။\n",
            "\n",
            "ဘာဖြစ်လို့ ဒီလို လွတ်လပ်ရေး မရမီကာလက လုပ်ဆောင်ခဲ့တဲ့ ပုဂ္ဂိုလ်ကြီးများ၊ ကျောင်းသားကြီးများရဲ့ ကဏ္ဍကို အဓိကထားပြီးတော့ ဖော်ပြရသလဲဆိုတော့ ဒီနေ့ဟာ အမျိုးသား အောင်ပွဲနေ့ဖြစ်ပါတယ်။ လွတ်လပ်ရေး အတွက် လုပ်ခဲ့တဲ့ ပုဂ္ဂိုလ်ကြီးများဟာတော့ အောင်ပွဲခံပြီးပါပြီ။\n",
            "\n",
            "ဒါကြောင့်မို့လို့ အမျိုးသားအောင်ပွဲနေ့ဟာ လွတ်လပ် ရေးရအောင် လုပ်ခဲ့တဲ့ ကျောင်းသားကြီးများ၊ ပုဂ္ဂိုလ်ကြီးများရဲ့ အဓိက ကဏ္ဍကို ဖေါ်ပြတဲ့နေ့လို့ ကျွန်မတို့ သတ်မှတ်ပါတယ်။ လွတ်လပ်ရေး နောက်ပိုင်းကာလမှာ ပေါက်ဖွားလာတဲ့ ကျွန်မတို့ တစ်တွေအတွက်တော့ တာဝန်ဟာ မပြီးသေးပါဘူး။ ကျွန်မတို့ဟာ လွတ်လပ်ရေးရရုံနဲ့ မပြီးသေးပါဘူး။ နိုင်ငံ တစ်နိုင်ငံဟာ လွတ်လပ်ရေးရပြီးတဲ့အခါမှာ ငြိမ်းချမ်းသာယာဖို့ လိုပါတယ်။ တိုးတက်ဖွံ့ဖြိုးဖို့ လိုပါတယ်။ နိုင်ငံသူ နိုင်ငံသား အားလုံးဟာ ဘဝလုံခြုံရေး ရှိပြီးတော့၊ စည်းလုံးညီညွတ်မှုလဲ ရဖို့ လိုပါတယ်။ ဒီတာဝန်တွေကို ဆက်ပြီးတော့ လုပ်ရမှာ ကျွန်မတို့ ၁၉၄၈ ခုနှစ် နောက်ပိုင်း လွတ်လပ်ရေးပြီးကာလ ကြီးပြင်းလာတဲ့ လူငယ်များ၊ လူ လတ်များရဲ့ တာဝန်ဖြစ်ပါတယ်လို့ ကျွန်မ အလေးအနက် ပြောချင်ပါတယ်။\n",
            "\n",
            "စောစောကပြောသွားတဲ့အထဲမှာ လွတ်လပ်ရေးမရမီ ကာလတုန်းကတော့ နိုင်ငံလွတ်လပ်မှ ပညာရေး လွတ်လပ်မယ်လို့ ပြောကြပါတယ်။ ဒီတော့ ကျွန်မတို့ အခု လွတ်လပ်ရေးရပြီးကာလမှာ ကျွန်မတို့အားလုံး နားလည်တာက တော့ ပညာရေးစင်စစ် လွတ်လပ်မှ နို်င်ငံသူ နိုင်ငံသားများလဲ လွတ်လပ်မယ်ဆိုတဲ့ ကိစ္စပါဘဲ။\n",
            "\n",
            "ဒီနေရာမှာ ကျွန်မတို့ဟာ ပညာဆိုတာဟာ ကျောင်းပညာကို တက္ကသိုလ်ပညာလောက်ကိုသာ ပြောတာ မဟုတ်ဘူး။ ပြည်သူလူထု ဗဟုသုတ ကြွယ်ဝစေမယ့် ကိစ္စအဝဝကို ပြောတာပါ။ ဒါကြောင့် ပညာရေး လွတ်လပ်မှုဆိုတဲ့အထဲမှာ လွတ်လပ်စွာ ပြောဆိုပိုင်ခွင့်၊ လွတ်လပ်စွာ ရေးသားပိုင်ခွင့်၊ လွတ်လပ်စွာ ထုတ်ဖော်ပိုင်ခွင့် ဆိုတာတွေ အားလုံးပါဖို့ လိုပါတယ်။ ဒီလို အခွင့်အရေးတွေ ရှိမှသာလျှင် ကျွန်မတို့ဟာ စင်စစ် လွတ်လပ်တဲ့ ပညာရေးစနစ်ကြောင့်မို့လို့ နိုင်ငံသူ နိုင်ငံသားများဟာ တကယ့်ကို လွတ်လပ်တယ်လို့ ကျွန်မတို့ ပြောနိုင်မှာ ဖြစ်ပါတယ်။ လွတ်လပ်ရေးရပြီးကာလ၊ ကျောင်းသူကျောင်းသားများရဲ့ ကဏ္ဍဟာ မလွယ်ပါဘူး။ တနည်းအားဖြင့် ဆိုလို့ရှိရင်၊ လွတ်လပ်ရေး မရမီကာလက ကျောင်းသားကျောင်းသူများရဲ့ အခြေအနေထက် ပိုပြီးတော့ ဆိုးတယ်လို့လဲ ဆိုနိုင်တယ်။ လုပ်ရမယ့် တာဝန်တွေက ပိုပြီးတော့ ခက်ခဲတယ်လို့လဲ ကျွန်မတို့ ဆိုနိုင်ပါတယ်။\n",
            "\n",
            "လွတ်လပ်ရေးမရခင်တုန်းက ကျောင်းသူ ကျောင်းသား များဟာ ကျောင်းသားသမဂ္ဂကနေ တစ်ဆင့် လှုပ်ရှားနိုင်တယ်။ လွတ်လပ်ရေးရပြီးမှ ကျောင်းသား သမဂ္ဂကြီး ပြိုကွဲသွားတယ်။ ဒါပေမယ့် ကျွန်မတို့ကျောင်းသား လူငယ်များရဲ့ နိုးကြားတက်ကြွတဲ့ စိတ်ကိုတော့ ဖြိုခွဲလို့ မရပါဘူး။ ဆက်လက် ရှင်သန်နေဆဲဖြစ်မယ်လို့ ကျွန်မ ယုံကြည်ပါတယ်။\n",
            "\n",
            "ကျွန်မတို့ရဲ့ အခု အမျိုးသားအောင်ပွဲနေ့ အထိမ်း အမှတ်ကတော့ ကဒေါင်းပါပဲ။ ဒါပေမယ့် ကျွန်မတို့ လက်ရှိ ကျောင်းသားများရဲ့ လှုပ်ရှားမှုရဲ့ အထိမ်းအမှတ်ကတော့ ခွပ်ဒေါင်းဖြစ်တယ်။\n",
            "\n",
            "ဒေါင်းဆိုတာ က,သင့်တဲ့အချိန်မှာ က,ရပါတယ်။ ခွပ်သင့်တဲ့အချိန်မှာ ခွပ်ရပါတယ်။ တချိန်လုံး က နေလို့လဲ မဖြစ်ဘူး။ တချိန်လုံး ခွပ်နေလို့လဲ မဖြစ်ဘူး။\n",
            "\n",
            "ဘယ်အချိန်မှာ ခွပ်သင့်တယ် ဆိုတာကို ပညာရှိ ပီပီ သိဖို့လိုပါတယ်။ ဒီတော့ ကျွန်မတို့ ကျောင်းသူကျောင်းသား များဟာ လွပ်လပ်ရေးရပြီးမှ အင်မတန်မှ ခွပ်ဒေါင်းအလုပ်များ လုပ်ခဲ့ကြရပါတယ်။ ခွပ်ဒေါင်းအလုပ်များ လုပ်ခဲ့ရတာဟာ တနေ့မှာ ကျွန်မတို့ ကဒေါင်းများလို တင့်တင့်တယ်တယ်နဲ့ ကြည်ကြည်နူးနူ း နေနိုင်ဖို့ဆိုတဲ့ ရည်ရွယ်ချက်နဲ့ ခွပ်ဒေါင်း အလုပ်တွေကို လုပ်နေရတာ ဖြစ်ပါတယ်။\n",
            "\n",
            "ခွပ်တဲ့ကိစ္စဆိုတာ ဘယ်သူမှ လိုလားတာ မဟုတ်ပါဘူး။ ပင်ပန်းပါတယ်။ ဆင်းရဲတယ်။ အများအတွက်လဲ ဒုက္ခ ရောက်တယ်၊ ဒါကြောင့်မို့လို့ ကျွန်မတို့ လူငယ်ကျောင်းသား များဟာ ခွပ်ဒေါင်းများ လုပ်ချင်လို့ လုပ်နေရတာ မဟုတ်ဘူး။ မလုပ်မဖြစ်လို့ လုပ်နေရတာလို့ ကျွန်မ ဒီလို မြင်ပါတယ်။\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train SentencePiece Tokenizer on Burmese Text this is the process of converting text into token\n",
        "\n",
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o0E8H6x00nY",
        "outputId": "aea41418-2677-4b32-d229-11a1f742c9da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input='allfiles_cleaned.txt',     # Your cleaned Burmese text file\n",
        "    model_prefix='burmese_tokenizer', # Output model files will start with this prefix\n",
        "    vocab_size=8000,                  # Number of tokens (adjust as needed) uniqure token that the model learn\n",
        "    character_coverage=0.995,         # Keep almost all Burmese characters\n",
        "    model_type='unigram',             # Recommended for Burmese script\n",
        "    max_sentence_length=10000         # Optional, increases max sentence length processed\n",
        ")\n",
        "print(\"Tokenizer training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhnTjOIQ1eRX",
        "outputId": "6ddc1dae-bc5e-42fb-8f2b-6d9fda18137b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Create a processor instance\n",
        "sp = spm.SentencePieceProcessor()\n",
        "\n",
        "# Load your trained tokenizer model\n",
        "sp.load('burmese_tokenizer.model')\n",
        "\n",
        "# Test text in Burmese\n",
        "text = \"မြန်မာဘာသာစကားကို စမ်းသပ်ပါ။\"\n",
        "\n",
        "# Encode text into tokens (as strings)\n",
        "tokens = sp.encode(text, out_type=str)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpwk0TXL2Gcv",
        "outputId": "165cdd92-2544-4eec-dbad-f0f39714cd6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['▁မြန်မာ', 'ဘာသာ', 'စကားကို', '▁စမ်းသပ်', 'ပါ။']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode tokens back to original text\n",
        "decoded_text = sp.decode(tokens)\n",
        "print(\"Decoded text:\", decoded_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRiBtUoz2apP",
        "outputId": "d9f18e5b-260b-45ae-db3b-94fb58725c3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded text: မြန်မာဘာသာစကားကို စမ်းသပ်ပါ။\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "##this is not need this is just to test anothe rparameter for more vocab\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input='allfiles_cleaned.txt',     # Your cleaned Burmese text file\n",
        "    model_prefix='burmese_tokenizer', # Output model files will start with this prefix\n",
        "    vocab_size=12000,                  # Number of tokens (adjust as needed) uniqure token that the model learn\n",
        "    character_coverage=1,         # Keep almost all Burmese characters\n",
        "    model_type='unigram',             # Recommended for Burmese script\n",
        "    max_sentence_length=10000         # Optional, increases max sentence length processed\n",
        ")\n",
        "print(\"Tokenizer training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afOxONYa2vVQ",
        "outputId": "178544a4-3977-41b3-8cc6-fc2fb3f917bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Create a processor instance\n",
        "sp = spm.SentencePieceProcessor()\n",
        "\n",
        "# Load your trained tokenizer model\n",
        "sp.load('burmese_tokenizer.model')\n",
        "\n",
        "# Test text in Burmese\n",
        "text = \"ဒီလို လွတ်လပ်ရေး မရမီကာလက လုပ်ဆောင်ခဲ့တဲ့ ပုဂ္ဂိုလ်ကြီးများ၊ ကျောင်းသားကြီးများရဲ့ ကဏ္ဍကို အဓိကထားပြီးတော့ ဖော်ပြရသလဲဆိုတော့ ဒီနေ့ဟာ အမျိုးသား အောင်ပွဲနေ့ဖြစ်ပါတယ်။ လွတ်လပ်ရေး အတွက် လုပ်ခဲ့တဲ့ ပုဂ္ဂိုလ်ကြီးများဟာတော့ အောင်ပွဲခံပြီးပါပြီ။\"\n",
        "\n",
        "# Encode text into tokens (as strings)\n",
        "tokens = sp.encode(text, out_type=str)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXcJoRye32ly",
        "outputId": "df8e017e-fda6-44e8-eaf7-a628806e98bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['▁ဒီလို', '▁လွတ်လပ်ရေး', '▁မရ', 'မီ', 'ကာလ', 'က', '▁', 'လုပ်ဆောင်', 'ခဲ့တဲ့', '▁ပုဂ္ဂိုလ်', 'ကြီးများ', '၊', '▁ကျောင်းသား', 'ကြီးများ', 'ရဲ့', '▁', 'က', 'ဏ္', 'ဍ', 'ကို', '▁အဓိက', 'ထားပြီး', 'တော့', '▁ဖော်ပြ', 'ရ', 'သလဲဆိုတော့', '▁ဒီနေ့', 'ဟာ', '▁အမျိုးသား', '▁', 'အောင်ပွဲနေ့', 'ဖြစ်ပါတယ်။', '▁လွတ်လပ်ရေး', '▁အတွက်', '▁လုပ်', 'ခဲ့တဲ့', '▁ပုဂ္ဂိုလ်', 'ကြီး', 'များဟာ', 'တော့', '▁အောင်', 'ပွဲ', 'ခံပြီး', 'ပါပြီ။']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Load your tokenizer\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('burmese_tokenizer.model')\n",
        "\n",
        "# Open input and output files\n",
        "with open(\"allfiles_cleaned.txt\", \"r\", encoding=\"utf-8\") as fin, \\\n",
        "     open(\"allfiles_tokenized.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
        "\n",
        "    for line in fin:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Encode line to token IDs (integers)\n",
        "        token_ids = sp.encode(line, out_type=int)\n",
        "\n",
        "        # Save token IDs as space-separated string\n",
        "        fout.write(\" \".join(map(str, token_ids)) + \"\\n\")\n",
        "\n",
        "print(\"Tokenization complete. Saved token IDs to allfiles_tokenized.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8uMf9dC4eof",
        "outputId": "c45029bd-66e8-44be-bf81-865df7c1d9ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete. Saved token IDs to allfiles_tokenized.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and preview first 5 lines of your tokenized file\n",
        "with open(\"allfiles_tokenized.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(5):\n",
        "        print(f.readline())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--ZzxE6i5Ger",
        "outputId": "734df84c-7506-43e7-bea6-7e0d7b2c0110"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21 140 9 3292 150 30 301 397 4917 35 25 2326 1179 28 1745 34 1538 10 4694 111 2589 1593 1962 385 1266 362 3 6622 3 3758 649 1138 1430 5 6380 7 144 649 2056 1430 42 3 8 492 0 7 1264 3875 514\n",
            "\n",
            "2320 237 2397 1266 362 1456 8 3 3758 649 2056 1430 5 1138 1430 42 3 8 492 0 7 2589 1159 16 1264 6 4593 1211 36 3161 3 6744 1797 2397 866 144 649 2056 35 1484 16 719 378 3259 1278\n",
            "\n",
            "4995 32 3161 6744 36 1962 385 490 144 649 1138 1430 5 2056 1430 42 2589 3 8 492 0 7 3 6578 142 3875 32 265 3 2745 100 2397 3 2328 1456 12 1302 1610 58 1065 265 134 2986 16 1117 36 6688 380 265 36 2397 6 366 19 6688 380 914 5787 36 2397 6 20 1394 3 2353 3476 76 3 4221 1405 4799 76 3 4221 914 59 3 5057 3 4657 793 4322 84 148 898 5 1912 178 2380 110 40 3 6 76 3 4221 81 2357 107 3136 16 3 6653 265 74 0 826 3 2328 2397 20 1456 7908 28 5855 5 79 3 1305 2506 1117 73 5507 219 7625 6989\n",
            "\n",
            "2634 99 39 6195 2397 1016 362 2631 562 16 914 5217 23 3174 1962 1078 93 961 610 265 424 2397 6 20 1456 12 265 1048 773 875 3 16 3174 3482 1962 23 905 330 4 396 238 270 59 914 1735 40 1962 4744 871 2471\n",
            "\n",
            "5303 265 36 657 4494 258 540 7 2046 540 136 886 2777 1607 1896 3 4677 3 4982 103 438 871 7676 330 93 1153 593 3174 1962 110 101 6195 1962 124 1340 6588 5 1962 124 1716 6588 5 1962 124 5642 6588 766 31 554 29 76 3 4221 237 3719 31 148 23 2105 265 36 3 3482 1962 28 3174 1187 231 436 32 914 59 3 5057 1484 1851 7 1962 320 265 93 50 12 514 2397 6 20 1456 5 3435 2425 2506 3 8 492 0 36 4874 380 119 201 1593 3 4136 5 2397 1266 362 1456 8 1138 327 556 42 1469 164 1542 16 3352 320 40 5667 67 144 1580 1117 268 1542 16 3 3807 320 40 265 5667 100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"allfiles_tokenized.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "kmpKvqjL5KdR",
        "outputId": "9dcf5a89-9015-4370-c8e9-df9632fba403"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_afd22a53-c7c7-4a91-844b-aa93adfe0d94\", \"allfiles_tokenized.txt\", 2798233)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing to Pretrain a Burmese Language Model\n"
      ],
      "metadata": {
        "id": "MTQVOAJq5cUV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Tokenized Data into a Single Long Tensor\n",
        "\n",
        "import torch\n",
        "\n",
        "all_token_ids = []\n",
        "\n",
        "with open(\"allfiles_tokenized.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        tokens = list(map(int, line.strip().split()))\n",
        "        all_token_ids.extend(tokens)\n",
        "\n",
        "data = torch.tensor(all_token_ids, dtype=torch.long)\n",
        "torch.save(data, \"burmese_dataset.pt\")\n",
        "\n",
        "print(\"Saved tokenized data to burmese_dataset.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqWaBF5P_NsH",
        "outputId": "1fe35c51-2afa-4fec-b368-3033d324fcc0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tokenized data to burmese_dataset.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create Training Dataset\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TokenDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx+self.block_size]\n",
        "        y = self.data[idx+1:idx+1+self.block_size]\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "OZ8AHWWLAIUU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definemodel\n",
        "\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=8000,  # Match your tokenizer vocab count\n",
        "    n_positions=1024,\n",
        "    n_ctx=1024,\n",
        "    n_embd=256,\n",
        "    n_layer=6,\n",
        "    n_head=4\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)\n"
      ],
      "metadata": {
        "id": "qvrJzKXGAfun"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "block_size = 128\n",
        "batch_size = 8\n",
        "dataset = TokenDataset(data, block_size)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "for epoch in range(5):  # Adjust epochs\n",
        "    model.train()\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(input_ids=x, labels=y)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Log every 50 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f\"Epoch {epoch+1} Step {step} Loss {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} finished. Last loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kPAQoq86PXK",
        "outputId": "91261438-9ec7-4d7f-ebd6-00a7a2ad2558"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Step 0 Loss 7.0634\n",
            "Epoch 1 Step 200 Loss 7.0989\n",
            "Epoch 1 Step 400 Loss 6.8654\n",
            "Epoch 1 Step 600 Loss 7.0248\n",
            "Epoch 1 Step 800 Loss 6.7162\n",
            "Epoch 1 Step 1000 Loss 6.8274\n",
            "Epoch 1 Step 1200 Loss 6.7508\n",
            "Epoch 1 Step 1400 Loss 6.7111\n",
            "Epoch 1 Step 1600 Loss 6.6361\n",
            "Epoch 1 Step 1800 Loss 6.7154\n",
            "Epoch 1 Step 2000 Loss 6.8850\n",
            "Epoch 1 Step 2200 Loss 6.4774\n",
            "Epoch 1 Step 2400 Loss 6.5413\n",
            "Epoch 1 Step 2600 Loss 6.5916\n",
            "Epoch 1 Step 2800 Loss 6.6266\n",
            "Epoch 1 Step 3000 Loss 6.8265\n",
            "Epoch 1 Step 3200 Loss 6.3768\n",
            "Epoch 1 Step 3400 Loss 6.7099\n",
            "Epoch 1 Step 3600 Loss 5.9975\n",
            "Epoch 1 Step 3800 Loss 6.2928\n",
            "Epoch 1 Step 4000 Loss 6.1748\n",
            "Epoch 1 Step 4200 Loss 6.1310\n",
            "Epoch 1 Step 4400 Loss 6.3509\n",
            "Epoch 1 Step 4600 Loss 6.3703\n",
            "Epoch 1 Step 4800 Loss 6.3004\n",
            "Epoch 1 Step 5000 Loss 6.6124\n",
            "Epoch 1 Step 5200 Loss 6.3489\n",
            "Epoch 1 Step 5400 Loss 6.3623\n",
            "Epoch 1 Step 5600 Loss 6.0176\n",
            "Epoch 1 Step 5800 Loss 6.4732\n",
            "Epoch 1 Step 6000 Loss 6.3971\n",
            "Epoch 1 Step 6200 Loss 5.9071\n",
            "Epoch 1 Step 6400 Loss 6.3971\n",
            "Epoch 1 Step 6600 Loss 6.5490\n",
            "Epoch 1 Step 6800 Loss 6.3399\n",
            "Epoch 1 Step 7000 Loss 6.0511\n",
            "Epoch 1 Step 7200 Loss 6.0852\n",
            "Epoch 1 Step 7400 Loss 6.2511\n",
            "Epoch 1 Step 7600 Loss 5.8299\n",
            "Epoch 1 Step 7800 Loss 6.2221\n",
            "Epoch 1 Step 8000 Loss 6.0261\n",
            "Epoch 1 Step 8200 Loss 6.1965\n",
            "Epoch 1 Step 8400 Loss 5.8888\n",
            "Epoch 1 Step 8600 Loss 6.1295\n",
            "Epoch 1 Step 8800 Loss 5.8770\n",
            "Epoch 1 Step 9000 Loss 5.7006\n",
            "Epoch 1 Step 9200 Loss 5.9727\n",
            "Epoch 1 Step 9400 Loss 6.0061\n",
            "Epoch 1 Step 9600 Loss 5.6353\n",
            "Epoch 1 Step 9800 Loss 5.6878\n",
            "Epoch 1 Step 10000 Loss 5.5977\n",
            "Epoch 1 Step 10200 Loss 6.0673\n",
            "Epoch 1 Step 10400 Loss 5.5996\n",
            "Epoch 1 Step 10600 Loss 5.9058\n",
            "Epoch 1 Step 10800 Loss 5.7802\n",
            "Epoch 1 Step 11000 Loss 5.7675\n",
            "Epoch 1 Step 11200 Loss 5.5684\n",
            "Epoch 1 Step 11400 Loss 5.8982\n",
            "Epoch 1 Step 11600 Loss 6.0309\n",
            "Epoch 1 Step 11800 Loss 5.7096\n",
            "Epoch 1 Step 12000 Loss 5.2763\n",
            "Epoch 1 Step 12200 Loss 5.4872\n",
            "Epoch 1 Step 12400 Loss 5.2337\n",
            "Epoch 1 Step 12600 Loss 5.4916\n",
            "Epoch 1 Step 12800 Loss 5.7942\n",
            "Epoch 1 Step 13000 Loss 5.5813\n",
            "Epoch 1 Step 13200 Loss 5.3738\n",
            "Epoch 1 Step 13400 Loss 5.4929\n",
            "Epoch 1 Step 13600 Loss 5.2483\n",
            "Epoch 1 Step 13800 Loss 5.4565\n",
            "Epoch 1 Step 14000 Loss 5.5043\n",
            "Epoch 1 Step 14200 Loss 5.3613\n",
            "Epoch 1 Step 14400 Loss 5.3256\n",
            "Epoch 1 Step 14600 Loss 5.5748\n",
            "Epoch 1 Step 14800 Loss 5.2568\n",
            "Epoch 1 Step 15000 Loss 5.2171\n",
            "Epoch 1 Step 15200 Loss 5.2048\n",
            "Epoch 1 Step 15400 Loss 5.1019\n",
            "Epoch 1 Step 15600 Loss 5.4406\n",
            "Epoch 1 Step 15800 Loss 5.2336\n",
            "Epoch 1 Step 16000 Loss 5.0343\n",
            "Epoch 1 Step 16200 Loss 5.1585\n",
            "Epoch 1 Step 16400 Loss 4.7440\n",
            "Epoch 1 Step 16600 Loss 5.0311\n",
            "Epoch 1 Step 16800 Loss 5.0642\n",
            "Epoch 1 Step 17000 Loss 5.0097\n",
            "Epoch 1 Step 17200 Loss 5.1144\n",
            "Epoch 1 Step 17400 Loss 4.9233\n",
            "Epoch 1 Step 17600 Loss 5.0318\n",
            "Epoch 1 Step 17800 Loss 4.8604\n",
            "Epoch 1 Step 18000 Loss 5.0710\n",
            "Epoch 1 Step 18200 Loss 5.1422\n",
            "Epoch 1 Step 18400 Loss 5.1302\n",
            "Epoch 1 Step 18600 Loss 5.4246\n",
            "Epoch 1 Step 18800 Loss 5.0484\n",
            "Epoch 1 Step 19000 Loss 4.5972\n",
            "Epoch 1 Step 19200 Loss 4.6067\n",
            "Epoch 1 Step 19400 Loss 4.6743\n",
            "Epoch 1 Step 19600 Loss 4.8001\n",
            "Epoch 1 Step 19800 Loss 4.8269\n",
            "Epoch 1 Step 20000 Loss 4.4712\n",
            "Epoch 1 Step 20200 Loss 4.7440\n",
            "Epoch 1 Step 20400 Loss 4.6751\n",
            "Epoch 1 Step 20600 Loss 4.6061\n",
            "Epoch 1 Step 20800 Loss 4.8491\n",
            "Epoch 1 Step 21000 Loss 5.0158\n",
            "Epoch 1 Step 21200 Loss 4.6214\n",
            "Epoch 1 Step 21400 Loss 5.2701\n",
            "Epoch 1 Step 21600 Loss 5.1651\n",
            "Epoch 1 Step 21800 Loss 5.0240\n",
            "Epoch 1 Step 22000 Loss 4.4942\n",
            "Epoch 1 Step 22200 Loss 4.1304\n",
            "Epoch 1 Step 22400 Loss 4.8076\n",
            "Epoch 1 Step 22600 Loss 4.9146\n",
            "Epoch 1 Step 22800 Loss 4.3322\n",
            "Epoch 1 Step 23000 Loss 4.3300\n",
            "Epoch 1 Step 23200 Loss 4.7703\n",
            "Epoch 1 Step 23400 Loss 4.6992\n",
            "Epoch 1 Step 23600 Loss 4.3491\n",
            "Epoch 1 Step 23800 Loss 4.8508\n",
            "Epoch 1 Step 24000 Loss 4.4979\n",
            "Epoch 1 Step 24200 Loss 4.7631\n",
            "Epoch 1 Step 24400 Loss 4.4684\n",
            "Epoch 1 Step 24600 Loss 4.5848\n",
            "Epoch 1 Step 24800 Loss 4.4320\n",
            "Epoch 1 Step 25000 Loss 4.7135\n",
            "Epoch 1 Step 25200 Loss 4.8629\n",
            "Epoch 1 Step 25400 Loss 5.0678\n",
            "Epoch 1 Step 25600 Loss 4.4270\n",
            "Epoch 1 Step 25800 Loss 4.3990\n",
            "Epoch 1 Step 26000 Loss 4.8604\n",
            "Epoch 1 Step 26200 Loss 4.8292\n",
            "Epoch 1 Step 26400 Loss 4.6173\n",
            "Epoch 1 Step 26600 Loss 4.7384\n",
            "Epoch 1 Step 26800 Loss 3.9380\n",
            "Epoch 1 Step 27000 Loss 4.6242\n",
            "Epoch 1 Step 27200 Loss 4.0890\n",
            "Epoch 1 Step 27400 Loss 4.5203\n",
            "Epoch 1 Step 27600 Loss 4.5852\n",
            "Epoch 1 Step 27800 Loss 4.7155\n",
            "Epoch 1 Step 28000 Loss 3.9591\n",
            "Epoch 1 Step 28200 Loss 4.7499\n",
            "Epoch 1 Step 28400 Loss 4.4802\n",
            "Epoch 1 Step 28600 Loss 3.9848\n",
            "Epoch 1 Step 28800 Loss 4.3359\n",
            "Epoch 1 Step 29000 Loss 3.7877\n",
            "Epoch 1 Step 29200 Loss 3.9300\n",
            "Epoch 1 Step 29400 Loss 4.4986\n",
            "Epoch 1 Step 29600 Loss 4.4109\n",
            "Epoch 1 Step 29800 Loss 4.1456\n",
            "Epoch 1 Step 30000 Loss 4.5901\n",
            "Epoch 1 Step 30200 Loss 4.1224\n",
            "Epoch 1 Step 30400 Loss 4.6918\n",
            "Epoch 1 Step 30600 Loss 4.3988\n",
            "Epoch 1 Step 30800 Loss 3.8868\n",
            "Epoch 1 Step 31000 Loss 4.2730\n",
            "Epoch 1 Step 31200 Loss 4.3727\n",
            "Epoch 1 Step 31400 Loss 4.0940\n",
            "Epoch 1 Step 31600 Loss 3.9971\n",
            "Epoch 1 Step 31800 Loss 4.1909\n",
            "Epoch 1 Step 32000 Loss 4.5090\n",
            "Epoch 1 Step 32200 Loss 4.3989\n",
            "Epoch 1 Step 32400 Loss 4.3531\n",
            "Epoch 1 Step 32600 Loss 3.9325\n",
            "Epoch 1 Step 32800 Loss 4.1426\n",
            "Epoch 1 Step 33000 Loss 4.2500\n",
            "Epoch 1 Step 33200 Loss 4.4234\n",
            "Epoch 1 Step 33400 Loss 4.0984\n",
            "Epoch 1 Step 33600 Loss 4.0159\n",
            "Epoch 1 Step 33800 Loss 3.7038\n",
            "Epoch 1 Step 34000 Loss 4.0264\n",
            "Epoch 1 Step 34200 Loss 3.9403\n",
            "Epoch 1 Step 34400 Loss 3.7520\n",
            "Epoch 1 Step 34600 Loss 3.8344\n",
            "Epoch 1 Step 34800 Loss 4.0689\n",
            "Epoch 1 Step 35000 Loss 4.1359\n",
            "Epoch 1 Step 35200 Loss 4.0232\n",
            "Epoch 1 Step 35400 Loss 4.3572\n",
            "Epoch 1 Step 35600 Loss 3.9378\n",
            "Epoch 1 Step 35800 Loss 4.1601\n",
            "Epoch 1 Step 36000 Loss 3.9407\n",
            "Epoch 1 Step 36200 Loss 4.3035\n",
            "Epoch 1 Step 36400 Loss 3.9340\n",
            "Epoch 1 Step 36600 Loss 4.4967\n",
            "Epoch 1 Step 36800 Loss 4.0072\n",
            "Epoch 1 Step 37000 Loss 3.9928\n",
            "Epoch 1 Step 37200 Loss 3.9215\n",
            "Epoch 1 Step 37400 Loss 3.7522\n",
            "Epoch 1 Step 37600 Loss 3.7923\n",
            "Epoch 1 Step 37800 Loss 4.2080\n",
            "Epoch 1 Step 38000 Loss 4.1624\n",
            "Epoch 1 Step 38200 Loss 3.5482\n",
            "Epoch 1 Step 38400 Loss 3.9602\n",
            "Epoch 1 Step 38600 Loss 4.1533\n",
            "Epoch 1 Step 38800 Loss 3.8940\n",
            "Epoch 1 Step 39000 Loss 3.5546\n",
            "Epoch 1 Step 39200 Loss 4.2657\n",
            "Epoch 1 Step 39400 Loss 4.1714\n",
            "Epoch 1 Step 39600 Loss 3.6051\n",
            "Epoch 1 Step 39800 Loss 3.7300\n",
            "Epoch 1 Step 40000 Loss 3.9877\n",
            "Epoch 1 Step 40200 Loss 3.8511\n",
            "Epoch 1 Step 40400 Loss 3.6539\n",
            "Epoch 1 Step 40600 Loss 4.0579\n",
            "Epoch 1 Step 40800 Loss 4.0332\n",
            "Epoch 1 Step 41000 Loss 3.8694\n",
            "Epoch 1 Step 41200 Loss 4.0860\n",
            "Epoch 1 Step 41400 Loss 4.0368\n",
            "Epoch 1 Step 41600 Loss 3.7843\n",
            "Epoch 1 Step 41800 Loss 3.7446\n",
            "Epoch 1 Step 42000 Loss 4.2054\n",
            "Epoch 1 Step 42200 Loss 4.0305\n",
            "Epoch 1 Step 42400 Loss 3.8006\n",
            "Epoch 1 Step 42600 Loss 4.1667\n",
            "Epoch 1 Step 42800 Loss 3.8174\n",
            "Epoch 1 Step 43000 Loss 4.1486\n",
            "Epoch 1 Step 43200 Loss 3.8015\n",
            "Epoch 1 Step 43400 Loss 3.4313\n",
            "Epoch 1 Step 43600 Loss 4.0332\n",
            "Epoch 1 Step 43800 Loss 3.9411\n",
            "Epoch 1 Step 44000 Loss 3.7988\n",
            "Epoch 1 Step 44200 Loss 3.6099\n",
            "Epoch 1 Step 44400 Loss 4.0642\n",
            "Epoch 1 Step 44600 Loss 3.5960\n",
            "Epoch 1 Step 44800 Loss 3.8806\n",
            "Epoch 1 Step 45000 Loss 4.0764\n",
            "Epoch 1 Step 45200 Loss 3.8532\n",
            "Epoch 1 Step 45400 Loss 3.3382\n",
            "Epoch 1 Step 45600 Loss 3.9876\n",
            "Epoch 1 Step 45800 Loss 3.3747\n",
            "Epoch 1 Step 46000 Loss 3.7724\n",
            "Epoch 1 Step 46200 Loss 3.8949\n",
            "Epoch 1 Step 46400 Loss 3.6360\n",
            "Epoch 1 Step 46600 Loss 3.5731\n",
            "Epoch 1 Step 46800 Loss 3.8595\n",
            "Epoch 1 Step 47000 Loss 3.6388\n",
            "Epoch 1 Step 47200 Loss 4.0378\n",
            "Epoch 1 Step 47400 Loss 3.9097\n",
            "Epoch 1 Step 47600 Loss 3.6239\n",
            "Epoch 1 Step 47800 Loss 4.0666\n",
            "Epoch 1 Step 48000 Loss 3.8603\n",
            "Epoch 1 Step 48200 Loss 3.6865\n",
            "Epoch 1 Step 48400 Loss 3.5354\n",
            "Epoch 1 Step 48600 Loss 3.6864\n",
            "Epoch 1 Step 48800 Loss 3.9314\n",
            "Epoch 1 Step 49000 Loss 3.8120\n",
            "Epoch 1 Step 49200 Loss 3.6985\n",
            "Epoch 1 Step 49400 Loss 3.6262\n",
            "Epoch 1 Step 49600 Loss 3.9660\n",
            "Epoch 1 Step 49800 Loss 3.7600\n",
            "Epoch 1 Step 50000 Loss 3.4131\n",
            "Epoch 1 Step 50200 Loss 3.8768\n",
            "Epoch 1 Step 50400 Loss 3.3853\n",
            "Epoch 1 Step 50600 Loss 3.7298\n",
            "Epoch 1 Step 50800 Loss 3.2853\n",
            "Epoch 1 Step 51000 Loss 3.4162\n",
            "Epoch 1 Step 51200 Loss 3.5048\n",
            "Epoch 1 Step 51400 Loss 3.6421\n",
            "Epoch 1 Step 51600 Loss 3.5656\n",
            "Epoch 1 Step 51800 Loss 3.7457\n",
            "Epoch 1 Step 52000 Loss 3.8709\n",
            "Epoch 1 Step 52200 Loss 3.3293\n",
            "Epoch 1 Step 52400 Loss 3.9552\n",
            "Epoch 1 Step 52600 Loss 3.6329\n",
            "Epoch 1 Step 52800 Loss 3.7031\n",
            "Epoch 1 Step 53000 Loss 3.4790\n",
            "Epoch 1 Step 53200 Loss 3.3522\n",
            "Epoch 1 Step 53400 Loss 3.9176\n",
            "Epoch 1 Step 53600 Loss 3.4902\n",
            "Epoch 1 Step 53800 Loss 3.6167\n",
            "Epoch 1 Step 54000 Loss 3.5496\n",
            "Epoch 1 Step 54200 Loss 3.1273\n",
            "Epoch 1 Step 54400 Loss 3.9679\n",
            "Epoch 1 Step 54600 Loss 3.7987\n",
            "Epoch 1 Step 54800 Loss 3.9669\n",
            "Epoch 1 Step 55000 Loss 3.2154\n",
            "Epoch 1 Step 55200 Loss 3.8418\n",
            "Epoch 1 Step 55400 Loss 3.3010\n",
            "Epoch 1 Step 55600 Loss 3.5197\n",
            "Epoch 1 Step 55800 Loss 3.7546\n",
            "Epoch 1 Step 56000 Loss 3.7333\n",
            "Epoch 1 Step 56200 Loss 3.5940\n",
            "Epoch 1 Step 56400 Loss 3.7043\n",
            "Epoch 1 Step 56600 Loss 3.5888\n",
            "Epoch 1 Step 56800 Loss 3.1522\n",
            "Epoch 1 Step 57000 Loss 3.5615\n",
            "Epoch 1 Step 57200 Loss 3.2044\n",
            "Epoch 1 Step 57400 Loss 3.1301\n",
            "Epoch 1 Step 57600 Loss 3.5941\n",
            "Epoch 1 Step 57800 Loss 3.4673\n",
            "Epoch 1 Step 58000 Loss 3.1169\n",
            "Epoch 1 Step 58200 Loss 3.3398\n",
            "Epoch 1 Step 58400 Loss 3.4577\n",
            "Epoch 1 Step 58600 Loss 3.7907\n",
            "Epoch 1 Step 58800 Loss 3.6187\n",
            "Epoch 1 Step 59000 Loss 3.3416\n",
            "Epoch 1 Step 59200 Loss 3.8599\n",
            "Epoch 1 Step 59400 Loss 3.2592\n",
            "Epoch 1 Step 59600 Loss 3.3583\n",
            "Epoch 1 Step 59800 Loss 3.1606\n",
            "Epoch 1 Step 60000 Loss 3.8867\n",
            "Epoch 1 Step 60200 Loss 3.5195\n",
            "Epoch 1 Step 60400 Loss 3.2574\n",
            "Epoch 1 Step 60600 Loss 3.2850\n",
            "Epoch 1 Step 60800 Loss 3.3933\n",
            "Epoch 1 Step 61000 Loss 2.9674\n",
            "Epoch 1 Step 61200 Loss 3.6858\n",
            "Epoch 1 Step 61400 Loss 3.1371\n",
            "Epoch 1 Step 61600 Loss 3.5617\n",
            "Epoch 1 Step 61800 Loss 3.8498\n",
            "Epoch 1 Step 62000 Loss 3.4889\n",
            "Epoch 1 Step 62200 Loss 3.1678\n",
            "Epoch 1 Step 62400 Loss 3.5548\n",
            "Epoch 1 Step 62600 Loss 3.6326\n",
            "Epoch 1 Step 62800 Loss 3.5342\n",
            "Epoch 1 Step 63000 Loss 3.6011\n",
            "Epoch 1 Step 63200 Loss 3.0036\n",
            "Epoch 1 Step 63400 Loss 3.5414\n",
            "Epoch 1 Step 63600 Loss 3.7114\n",
            "Epoch 1 Step 63800 Loss 3.2573\n",
            "Epoch 1 Step 64000 Loss 3.4651\n",
            "Epoch 1 Step 64200 Loss 3.3647\n",
            "Epoch 1 Step 64400 Loss 3.7045\n",
            "Epoch 1 Step 64600 Loss 3.3982\n",
            "Epoch 1 Step 64800 Loss 3.5671\n",
            "Epoch 1 Step 65000 Loss 3.3658\n",
            "Epoch 1 Step 65200 Loss 3.6421\n",
            "Epoch 1 Step 65400 Loss 3.4590\n",
            "Epoch 1 Step 65600 Loss 3.0915\n",
            "Epoch 1 Step 65800 Loss 3.2969\n",
            "Epoch 1 Step 66000 Loss 3.3348\n",
            "Epoch 1 Step 66200 Loss 3.6579\n",
            "Epoch 1 Step 66400 Loss 3.6065\n",
            "Epoch 1 Step 66600 Loss 3.6324\n",
            "Epoch 1 Step 66800 Loss 3.0386\n",
            "Epoch 1 Step 67000 Loss 3.4277\n",
            "Epoch 1 Step 67200 Loss 3.1188\n",
            "Epoch 1 Step 67400 Loss 3.4653\n",
            "Epoch 1 Step 67600 Loss 3.1453\n",
            "Epoch 1 Step 67800 Loss 3.2765\n",
            "Epoch 1 Step 68000 Loss 3.3651\n",
            "Epoch 1 Step 68200 Loss 3.2201\n",
            "Epoch 1 Step 68400 Loss 3.0880\n",
            "Epoch 1 Step 68600 Loss 3.3147\n",
            "Epoch 1 Step 68800 Loss 3.3896\n",
            "Epoch 1 Step 69000 Loss 3.2337\n",
            "Epoch 1 Step 69200 Loss 3.3395\n",
            "Epoch 1 Step 69400 Loss 3.4293\n",
            "Epoch 1 Step 69600 Loss 3.1669\n",
            "Epoch 1 Step 69800 Loss 3.1586\n",
            "Epoch 1 Step 70000 Loss 3.5568\n",
            "Epoch 1 Step 70200 Loss 3.4105\n",
            "Epoch 1 Step 70400 Loss 3.3055\n",
            "Epoch 1 Step 70600 Loss 3.5212\n",
            "Epoch 1 Step 70800 Loss 3.1652\n",
            "Epoch 1 Step 71000 Loss 3.4186\n",
            "Epoch 1 Step 71200 Loss 3.2843\n",
            "Epoch 1 Step 71400 Loss 2.9512\n",
            "Epoch 1 Step 71600 Loss 3.3472\n",
            "Epoch 1 Step 71800 Loss 3.5000\n",
            "Epoch 1 Step 72000 Loss 3.5953\n",
            "Epoch 1 Step 72200 Loss 3.3602\n",
            "Epoch 1 Step 72400 Loss 3.2178\n",
            "Epoch 1 Step 72600 Loss 3.5466\n",
            "Epoch 1 Step 72800 Loss 3.4948\n",
            "Epoch 1 Step 73000 Loss 3.1384\n",
            "Epoch 1 Step 73200 Loss 3.5170\n",
            "Epoch 1 Step 73400 Loss 3.2512\n",
            "Epoch 1 Step 73600 Loss 3.2517\n",
            "Epoch 1 Step 73800 Loss 3.4488\n",
            "Epoch 1 Step 74000 Loss 3.3742\n",
            "Epoch 1 Step 74200 Loss 3.4408\n",
            "Epoch 1 Step 74400 Loss 3.3489\n",
            "Epoch 1 Step 74600 Loss 3.0834\n",
            "Epoch 1 Step 74800 Loss 3.1486\n",
            "Epoch 1 Step 75000 Loss 3.2851\n",
            "Epoch 1 Step 75200 Loss 3.3106\n",
            "Epoch 1 Step 75400 Loss 3.7436\n",
            "Epoch 1 Step 75600 Loss 3.0564\n",
            "Epoch 1 Step 75800 Loss 3.6964\n",
            "Epoch 1 Step 76000 Loss 3.0061\n",
            "Epoch 1 Step 76200 Loss 3.2650\n",
            "Epoch 1 Step 76400 Loss 3.1502\n",
            "Epoch 1 Step 76600 Loss 3.4809\n",
            "Epoch 1 Step 76800 Loss 3.3092\n",
            "Epoch 1 Step 77000 Loss 3.4060\n",
            "Epoch 1 Step 77200 Loss 3.1041\n",
            "Epoch 1 Step 77400 Loss 3.6256\n",
            "Epoch 1 Step 77600 Loss 3.1084\n",
            "Epoch 1 Step 77800 Loss 3.6043\n",
            "Epoch 1 Step 78000 Loss 3.4296\n",
            "Epoch 1 Step 78200 Loss 3.3358\n",
            "Epoch 1 Step 78400 Loss 3.4249\n",
            "Epoch 1 Step 78600 Loss 3.1709\n",
            "Epoch 1 Step 78800 Loss 3.3959\n",
            "Epoch 1 Step 79000 Loss 3.0239\n",
            "Epoch 1 Step 79200 Loss 3.3723\n",
            "Epoch 1 Step 79400 Loss 3.2804\n",
            "Epoch 1 Step 79600 Loss 3.4510\n",
            "Epoch 1 Step 79800 Loss 3.3050\n",
            "Epoch 1 Step 80000 Loss 3.3444\n",
            "Epoch 1 Step 80200 Loss 3.3321\n",
            "Epoch 1 Step 80400 Loss 3.3894\n",
            "Epoch 1 Step 80600 Loss 3.1801\n",
            "Epoch 1 Step 80800 Loss 3.1842\n",
            "Epoch 1 Step 81000 Loss 3.3820\n",
            "Epoch 1 Step 81200 Loss 3.1061\n",
            "Epoch 1 Step 81400 Loss 2.9793\n",
            "Epoch 1 Step 81600 Loss 3.4188\n",
            "Epoch 1 Step 81800 Loss 3.4041\n",
            "Epoch 1 Step 82000 Loss 3.3198\n",
            "Epoch 1 Step 82200 Loss 3.2590\n",
            "Epoch 1 Step 82400 Loss 3.2938\n",
            "Epoch 1 Step 82600 Loss 2.8591\n",
            "Epoch 1 Step 82800 Loss 3.5644\n",
            "Epoch 1 Step 83000 Loss 3.1184\n",
            "Epoch 1 Step 83200 Loss 2.9869\n",
            "Epoch 1 Step 83400 Loss 2.9077\n",
            "Epoch 1 Step 83600 Loss 3.0938\n",
            "Epoch 1 Step 83800 Loss 3.2931\n",
            "Epoch 1 Step 84000 Loss 3.1297\n",
            "Epoch 1 Step 84200 Loss 3.2445\n",
            "Epoch 1 Step 84400 Loss 3.2691\n",
            "Epoch 1 Step 84600 Loss 3.1672\n",
            "Epoch 1 Step 84800 Loss 3.3103\n",
            "Epoch 1 Step 85000 Loss 3.1749\n",
            "Epoch 1 Step 85200 Loss 3.0800\n",
            "Epoch 1 Step 85400 Loss 3.1427\n",
            "Epoch 1 Step 85600 Loss 3.4496\n",
            "Epoch 1 Step 85800 Loss 3.0186\n",
            "Epoch 1 Step 86000 Loss 3.2799\n",
            "Epoch 1 Step 86200 Loss 3.2270\n",
            "Epoch 1 Step 86400 Loss 3.2765\n",
            "Epoch 1 Step 86600 Loss 3.0536\n",
            "Epoch 1 Step 86800 Loss 3.3455\n",
            "Epoch 1 Step 87000 Loss 3.4317\n",
            "Epoch 1 Step 87200 Loss 3.4707\n",
            "Epoch 1 Step 87400 Loss 3.4549\n",
            "Epoch 1 Step 87600 Loss 3.0951\n",
            "Epoch 1 Step 87800 Loss 3.3840\n",
            "Epoch 1 Step 88000 Loss 3.2500\n",
            "Epoch 1 Step 88200 Loss 3.0572\n",
            "Epoch 1 Step 88400 Loss 3.1685\n",
            "Epoch 1 Step 88600 Loss 3.1542\n",
            "Epoch 1 Step 88800 Loss 3.5392\n",
            "Epoch 1 Step 89000 Loss 3.5021\n",
            "Epoch 1 Step 89200 Loss 2.7754\n",
            "Epoch 1 Step 89400 Loss 3.2319\n",
            "Epoch 1 Step 89600 Loss 3.2163\n",
            "Epoch 1 Step 89800 Loss 3.3893\n",
            "Epoch 1 Step 90000 Loss 3.0146\n",
            "Epoch 1 Step 90200 Loss 3.0647\n",
            "Epoch 1 Step 90400 Loss 3.0622\n",
            "Epoch 1 Step 90600 Loss 3.3402\n",
            "Epoch 1 Step 90800 Loss 3.4915\n",
            "Epoch 1 Step 91000 Loss 3.0201\n",
            "Epoch 1 Step 91200 Loss 3.1621\n",
            "Epoch 1 Step 91400 Loss 3.0099\n",
            "Epoch 1 Step 91600 Loss 3.1587\n",
            "Epoch 1 finished. Last loss = 2.9556\n",
            "Epoch 2 Step 0 Loss 3.3648\n",
            "Epoch 2 Step 200 Loss 3.3463\n",
            "Epoch 2 Step 400 Loss 3.2135\n",
            "Epoch 2 Step 600 Loss 3.2945\n",
            "Epoch 2 Step 800 Loss 2.7386\n",
            "Epoch 2 Step 1000 Loss 2.9761\n",
            "Epoch 2 Step 1200 Loss 2.8018\n",
            "Epoch 2 Step 1400 Loss 3.4807\n",
            "Epoch 2 Step 1600 Loss 2.9204\n",
            "Epoch 2 Step 1800 Loss 3.0925\n",
            "Epoch 2 Step 2000 Loss 3.1959\n",
            "Epoch 2 Step 2200 Loss 2.8067\n",
            "Epoch 2 Step 2400 Loss 2.4603\n",
            "Epoch 2 Step 2600 Loss 3.0884\n",
            "Epoch 2 Step 2800 Loss 2.9472\n",
            "Epoch 2 Step 3000 Loss 3.0562\n",
            "Epoch 2 Step 3200 Loss 3.3444\n",
            "Epoch 2 Step 3400 Loss 3.0458\n",
            "Epoch 2 Step 3600 Loss 3.1018\n",
            "Epoch 2 Step 3800 Loss 3.1017\n",
            "Epoch 2 Step 4000 Loss 3.0661\n",
            "Epoch 2 Step 4200 Loss 3.0553\n",
            "Epoch 2 Step 4400 Loss 2.9060\n",
            "Epoch 2 Step 4600 Loss 3.1821\n",
            "Epoch 2 Step 4800 Loss 3.0158\n",
            "Epoch 2 Step 5000 Loss 3.2809\n",
            "Epoch 2 Step 5200 Loss 3.0855\n",
            "Epoch 2 Step 5400 Loss 3.2423\n",
            "Epoch 2 Step 5600 Loss 3.2632\n",
            "Epoch 2 Step 5800 Loss 2.9695\n",
            "Epoch 2 Step 6000 Loss 3.2010\n",
            "Epoch 2 Step 6200 Loss 3.4467\n",
            "Epoch 2 Step 6400 Loss 2.8695\n",
            "Epoch 2 Step 6600 Loss 3.0083\n",
            "Epoch 2 Step 6800 Loss 2.9075\n",
            "Epoch 2 Step 7000 Loss 2.9864\n",
            "Epoch 2 Step 7200 Loss 2.9859\n",
            "Epoch 2 Step 7400 Loss 3.1985\n",
            "Epoch 2 Step 7600 Loss 3.1863\n",
            "Epoch 2 Step 7800 Loss 2.7087\n",
            "Epoch 2 Step 8000 Loss 3.0662\n",
            "Epoch 2 Step 8200 Loss 3.1769\n",
            "Epoch 2 Step 8400 Loss 2.9568\n",
            "Epoch 2 Step 8600 Loss 3.1851\n",
            "Epoch 2 Step 8800 Loss 2.9010\n",
            "Epoch 2 Step 9000 Loss 3.1096\n",
            "Epoch 2 Step 9200 Loss 3.1683\n",
            "Epoch 2 Step 9400 Loss 2.8880\n",
            "Epoch 2 Step 9600 Loss 3.0424\n",
            "Epoch 2 Step 9800 Loss 3.1945\n",
            "Epoch 2 Step 10000 Loss 2.9923\n",
            "Epoch 2 Step 10200 Loss 2.9121\n",
            "Epoch 2 Step 10400 Loss 3.0946\n",
            "Epoch 2 Step 10600 Loss 2.8207\n",
            "Epoch 2 Step 10800 Loss 3.0464\n",
            "Epoch 2 Step 11000 Loss 3.1000\n",
            "Epoch 2 Step 11200 Loss 3.2346\n",
            "Epoch 2 Step 11400 Loss 2.8807\n",
            "Epoch 2 Step 11600 Loss 2.8058\n",
            "Epoch 2 Step 11800 Loss 2.9966\n",
            "Epoch 2 Step 12000 Loss 3.3529\n",
            "Epoch 2 Step 12200 Loss 3.0430\n",
            "Epoch 2 Step 12400 Loss 3.0761\n",
            "Epoch 2 Step 12600 Loss 2.7368\n",
            "Epoch 2 Step 12800 Loss 2.9729\n",
            "Epoch 2 Step 13000 Loss 3.2659\n",
            "Epoch 2 Step 13200 Loss 3.3121\n",
            "Epoch 2 Step 13400 Loss 3.1387\n",
            "Epoch 2 Step 13600 Loss 2.7963\n",
            "Epoch 2 Step 13800 Loss 3.1477\n",
            "Epoch 2 Step 14000 Loss 3.1338\n",
            "Epoch 2 Step 14200 Loss 3.1131\n",
            "Epoch 2 Step 14400 Loss 3.1627\n",
            "Epoch 2 Step 14600 Loss 2.9342\n",
            "Epoch 2 Step 14800 Loss 2.8033\n",
            "Epoch 2 Step 15000 Loss 2.9979\n",
            "Epoch 2 Step 15200 Loss 3.0664\n",
            "Epoch 2 Step 15400 Loss 2.9055\n",
            "Epoch 2 Step 15600 Loss 2.8762\n",
            "Epoch 2 Step 15800 Loss 2.9495\n",
            "Epoch 2 Step 16000 Loss 3.0393\n",
            "Epoch 2 Step 16200 Loss 3.0024\n",
            "Epoch 2 Step 16400 Loss 3.0233\n",
            "Epoch 2 Step 16600 Loss 3.1566\n",
            "Epoch 2 Step 16800 Loss 3.1924\n",
            "Epoch 2 Step 17000 Loss 2.8997\n",
            "Epoch 2 Step 17200 Loss 2.9844\n",
            "Epoch 2 Step 17400 Loss 3.0316\n",
            "Epoch 2 Step 17600 Loss 2.9285\n",
            "Epoch 2 Step 17800 Loss 3.5095\n",
            "Epoch 2 Step 18000 Loss 3.2366\n",
            "Epoch 2 Step 18200 Loss 2.9362\n",
            "Epoch 2 Step 18400 Loss 2.8978\n",
            "Epoch 2 Step 18600 Loss 3.0331\n",
            "Epoch 2 Step 18800 Loss 2.8933\n",
            "Epoch 2 Step 19000 Loss 2.8594\n",
            "Epoch 2 Step 19200 Loss 2.7665\n",
            "Epoch 2 Step 19400 Loss 3.0202\n",
            "Epoch 2 Step 19600 Loss 3.3341\n",
            "Epoch 2 Step 19800 Loss 3.4212\n",
            "Epoch 2 Step 20000 Loss 2.9967\n",
            "Epoch 2 Step 20200 Loss 3.0085\n",
            "Epoch 2 Step 20400 Loss 3.0162\n",
            "Epoch 2 Step 20600 Loss 3.0632\n",
            "Epoch 2 Step 20800 Loss 2.8956\n",
            "Epoch 2 Step 21000 Loss 2.8641\n",
            "Epoch 2 Step 21200 Loss 3.0502\n",
            "Epoch 2 Step 21400 Loss 2.9984\n",
            "Epoch 2 Step 21600 Loss 2.6131\n",
            "Epoch 2 Step 21800 Loss 3.2011\n",
            "Epoch 2 Step 22000 Loss 3.0020\n",
            "Epoch 2 Step 22200 Loss 3.0481\n",
            "Epoch 2 Step 22400 Loss 2.9025\n",
            "Epoch 2 Step 22600 Loss 3.1275\n",
            "Epoch 2 Step 22800 Loss 2.8419\n",
            "Epoch 2 Step 23000 Loss 2.8171\n",
            "Epoch 2 Step 23200 Loss 2.9072\n",
            "Epoch 2 Step 23400 Loss 3.0517\n",
            "Epoch 2 Step 23600 Loss 3.2794\n",
            "Epoch 2 Step 23800 Loss 2.8434\n",
            "Epoch 2 Step 24000 Loss 3.0404\n",
            "Epoch 2 Step 24200 Loss 2.8232\n",
            "Epoch 2 Step 24400 Loss 2.9817\n",
            "Epoch 2 Step 24600 Loss 2.9079\n",
            "Epoch 2 Step 24800 Loss 3.0712\n",
            "Epoch 2 Step 25000 Loss 2.8397\n",
            "Epoch 2 Step 25200 Loss 2.6437\n",
            "Epoch 2 Step 25400 Loss 3.0806\n",
            "Epoch 2 Step 25600 Loss 3.0923\n",
            "Epoch 2 Step 25800 Loss 2.9033\n",
            "Epoch 2 Step 26000 Loss 3.2487\n",
            "Epoch 2 Step 26200 Loss 2.9971\n",
            "Epoch 2 Step 26400 Loss 3.1178\n",
            "Epoch 2 Step 26600 Loss 3.1211\n",
            "Epoch 2 Step 26800 Loss 2.8827\n",
            "Epoch 2 Step 27000 Loss 2.6649\n",
            "Epoch 2 Step 27200 Loss 3.1498\n",
            "Epoch 2 Step 27400 Loss 3.0915\n",
            "Epoch 2 Step 27600 Loss 2.7034\n",
            "Epoch 2 Step 27800 Loss 3.2095\n",
            "Epoch 2 Step 28000 Loss 3.2082\n",
            "Epoch 2 Step 28200 Loss 2.5326\n",
            "Epoch 2 Step 28400 Loss 2.8008\n",
            "Epoch 2 Step 28600 Loss 3.0391\n",
            "Epoch 2 Step 28800 Loss 2.8798\n",
            "Epoch 2 Step 29000 Loss 2.9977\n",
            "Epoch 2 Step 29200 Loss 2.7228\n",
            "Epoch 2 Step 29400 Loss 3.1162\n",
            "Epoch 2 Step 29600 Loss 3.1142\n",
            "Epoch 2 Step 29800 Loss 3.1204\n",
            "Epoch 2 Step 30000 Loss 3.2351\n",
            "Epoch 2 Step 30200 Loss 3.0536\n",
            "Epoch 2 Step 30400 Loss 2.8514\n",
            "Epoch 2 Step 30600 Loss 2.9402\n",
            "Epoch 2 Step 30800 Loss 2.9300\n",
            "Epoch 2 Step 31000 Loss 2.7912\n",
            "Epoch 2 Step 31200 Loss 2.7830\n",
            "Epoch 2 Step 31400 Loss 2.9243\n",
            "Epoch 2 Step 31600 Loss 3.0717\n",
            "Epoch 2 Step 31800 Loss 2.9735\n",
            "Epoch 2 Step 32000 Loss 2.8486\n",
            "Epoch 2 Step 32200 Loss 2.9808\n",
            "Epoch 2 Step 32400 Loss 2.8232\n",
            "Epoch 2 Step 32600 Loss 3.1614\n",
            "Epoch 2 Step 32800 Loss 3.0858\n",
            "Epoch 2 Step 33000 Loss 2.6894\n",
            "Epoch 2 Step 33200 Loss 2.7282\n",
            "Epoch 2 Step 33400 Loss 2.7122\n",
            "Epoch 2 Step 33600 Loss 2.8468\n",
            "Epoch 2 Step 33800 Loss 3.1366\n",
            "Epoch 2 Step 34000 Loss 2.8427\n",
            "Epoch 2 Step 34200 Loss 2.6813\n",
            "Epoch 2 Step 34400 Loss 3.2711\n",
            "Epoch 2 Step 34600 Loss 2.7564\n",
            "Epoch 2 Step 34800 Loss 2.5781\n",
            "Epoch 2 Step 35000 Loss 3.2801\n",
            "Epoch 2 Step 35200 Loss 3.0475\n",
            "Epoch 2 Step 35400 Loss 3.2338\n",
            "Epoch 2 Step 35600 Loss 2.7452\n",
            "Epoch 2 Step 35800 Loss 3.0846\n",
            "Epoch 2 Step 36000 Loss 3.0211\n",
            "Epoch 2 Step 36200 Loss 2.5721\n",
            "Epoch 2 Step 36400 Loss 2.8140\n",
            "Epoch 2 Step 36600 Loss 2.9749\n",
            "Epoch 2 Step 36800 Loss 2.8897\n",
            "Epoch 2 Step 37000 Loss 2.8269\n",
            "Epoch 2 Step 37200 Loss 3.0208\n",
            "Epoch 2 Step 37400 Loss 2.7432\n",
            "Epoch 2 Step 37600 Loss 2.6301\n",
            "Epoch 2 Step 37800 Loss 3.1222\n",
            "Epoch 2 Step 38000 Loss 3.1310\n",
            "Epoch 2 Step 38200 Loss 2.8657\n",
            "Epoch 2 Step 38400 Loss 2.7691\n",
            "Epoch 2 Step 38600 Loss 3.1632\n",
            "Epoch 2 Step 38800 Loss 3.1176\n",
            "Epoch 2 Step 39000 Loss 2.6862\n",
            "Epoch 2 Step 39200 Loss 2.8695\n",
            "Epoch 2 Step 39400 Loss 2.7694\n",
            "Epoch 2 Step 39600 Loss 2.7849\n",
            "Epoch 2 Step 39800 Loss 2.8937\n",
            "Epoch 2 Step 40000 Loss 2.8839\n",
            "Epoch 2 Step 40200 Loss 2.8822\n",
            "Epoch 2 Step 40400 Loss 3.1712\n",
            "Epoch 2 Step 40600 Loss 2.9361\n",
            "Epoch 2 Step 40800 Loss 2.7781\n",
            "Epoch 2 Step 41000 Loss 2.8919\n",
            "Epoch 2 Step 41200 Loss 2.9481\n",
            "Epoch 2 Step 41400 Loss 2.4731\n",
            "Epoch 2 Step 41600 Loss 2.8479\n",
            "Epoch 2 Step 41800 Loss 2.9171\n",
            "Epoch 2 Step 42000 Loss 2.9393\n",
            "Epoch 2 Step 42200 Loss 2.9242\n",
            "Epoch 2 Step 42400 Loss 3.3278\n",
            "Epoch 2 Step 42600 Loss 3.2145\n",
            "Epoch 2 Step 42800 Loss 2.7716\n",
            "Epoch 2 Step 43000 Loss 2.7004\n",
            "Epoch 2 Step 43200 Loss 2.9652\n",
            "Epoch 2 Step 43400 Loss 2.9252\n",
            "Epoch 2 Step 43600 Loss 3.1314\n",
            "Epoch 2 Step 43800 Loss 2.9527\n",
            "Epoch 2 Step 44000 Loss 2.9852\n",
            "Epoch 2 Step 44200 Loss 3.2304\n",
            "Epoch 2 Step 44400 Loss 2.7137\n",
            "Epoch 2 Step 44600 Loss 2.9258\n",
            "Epoch 2 Step 44800 Loss 2.8164\n",
            "Epoch 2 Step 45000 Loss 2.9109\n",
            "Epoch 2 Step 45200 Loss 2.8988\n",
            "Epoch 2 Step 45400 Loss 3.0451\n",
            "Epoch 2 Step 45600 Loss 2.5966\n",
            "Epoch 2 Step 45800 Loss 2.5374\n",
            "Epoch 2 Step 46000 Loss 2.7569\n",
            "Epoch 2 Step 46200 Loss 2.9531\n",
            "Epoch 2 Step 46400 Loss 2.6763\n",
            "Epoch 2 Step 46600 Loss 2.5716\n",
            "Epoch 2 Step 46800 Loss 2.5816\n",
            "Epoch 2 Step 47000 Loss 2.8303\n",
            "Epoch 2 Step 47200 Loss 2.7326\n",
            "Epoch 2 Step 47400 Loss 2.8982\n",
            "Epoch 2 Step 47600 Loss 2.5916\n",
            "Epoch 2 Step 47800 Loss 2.2759\n",
            "Epoch 2 Step 48000 Loss 3.0354\n",
            "Epoch 2 Step 48200 Loss 2.5529\n",
            "Epoch 2 Step 48400 Loss 2.8577\n",
            "Epoch 2 Step 48600 Loss 3.0664\n",
            "Epoch 2 Step 48800 Loss 3.2168\n",
            "Epoch 2 Step 49000 Loss 3.0000\n",
            "Epoch 2 Step 49200 Loss 2.7466\n",
            "Epoch 2 Step 49400 Loss 2.7696\n",
            "Epoch 2 Step 49600 Loss 3.0603\n",
            "Epoch 2 Step 49800 Loss 2.7474\n",
            "Epoch 2 Step 50000 Loss 2.9290\n",
            "Epoch 2 Step 50200 Loss 2.6618\n",
            "Epoch 2 Step 50400 Loss 2.7625\n",
            "Epoch 2 Step 50600 Loss 3.1874\n",
            "Epoch 2 Step 50800 Loss 2.6576\n",
            "Epoch 2 Step 51000 Loss 2.7680\n",
            "Epoch 2 Step 51200 Loss 3.3096\n",
            "Epoch 2 Step 51400 Loss 2.7477\n",
            "Epoch 2 Step 51600 Loss 2.6824\n",
            "Epoch 2 Step 51800 Loss 2.6439\n",
            "Epoch 2 Step 52000 Loss 2.8399\n",
            "Epoch 2 Step 52200 Loss 2.8020\n",
            "Epoch 2 Step 52400 Loss 3.1142\n",
            "Epoch 2 Step 52600 Loss 2.6349\n",
            "Epoch 2 Step 52800 Loss 2.7291\n",
            "Epoch 2 Step 53000 Loss 2.7088\n",
            "Epoch 2 Step 53200 Loss 3.1736\n",
            "Epoch 2 Step 53400 Loss 3.0990\n",
            "Epoch 2 Step 53600 Loss 2.8077\n",
            "Epoch 2 Step 53800 Loss 3.2282\n",
            "Epoch 2 Step 54000 Loss 2.6032\n",
            "Epoch 2 Step 54200 Loss 3.0037\n",
            "Epoch 2 Step 54400 Loss 2.4884\n",
            "Epoch 2 Step 54600 Loss 2.7999\n",
            "Epoch 2 Step 54800 Loss 2.4743\n",
            "Epoch 2 Step 55000 Loss 2.6273\n",
            "Epoch 2 Step 55200 Loss 2.6763\n",
            "Epoch 2 Step 55400 Loss 3.0084\n",
            "Epoch 2 Step 55600 Loss 2.7859\n",
            "Epoch 2 Step 55800 Loss 2.6546\n",
            "Epoch 2 Step 56000 Loss 2.9538\n",
            "Epoch 2 Step 56200 Loss 2.9516\n",
            "Epoch 2 Step 56400 Loss 2.7543\n",
            "Epoch 2 Step 56600 Loss 3.0904\n",
            "Epoch 2 Step 56800 Loss 2.4337\n",
            "Epoch 2 Step 57000 Loss 2.7597\n",
            "Epoch 2 Step 57200 Loss 2.8289\n",
            "Epoch 2 Step 57400 Loss 3.1417\n",
            "Epoch 2 Step 57600 Loss 2.8897\n",
            "Epoch 2 Step 57800 Loss 2.8109\n",
            "Epoch 2 Step 58000 Loss 2.5237\n",
            "Epoch 2 Step 58200 Loss 2.8439\n",
            "Epoch 2 Step 58400 Loss 2.5362\n",
            "Epoch 2 Step 58600 Loss 2.8913\n",
            "Epoch 2 Step 58800 Loss 2.9030\n",
            "Epoch 2 Step 59000 Loss 2.7844\n",
            "Epoch 2 Step 59200 Loss 2.7362\n",
            "Epoch 2 Step 59400 Loss 2.9214\n",
            "Epoch 2 Step 59600 Loss 2.3171\n",
            "Epoch 2 Step 59800 Loss 3.0286\n",
            "Epoch 2 Step 60000 Loss 2.9651\n",
            "Epoch 2 Step 60200 Loss 2.7584\n",
            "Epoch 2 Step 60400 Loss 2.8846\n",
            "Epoch 2 Step 60600 Loss 2.9092\n",
            "Epoch 2 Step 60800 Loss 2.7457\n",
            "Epoch 2 Step 61000 Loss 2.5359\n",
            "Epoch 2 Step 61200 Loss 3.0464\n",
            "Epoch 2 Step 61400 Loss 2.5458\n",
            "Epoch 2 Step 61600 Loss 2.9344\n",
            "Epoch 2 Step 61800 Loss 2.9865\n",
            "Epoch 2 Step 62000 Loss 2.6491\n",
            "Epoch 2 Step 62200 Loss 2.5437\n",
            "Epoch 2 Step 62400 Loss 2.6487\n",
            "Epoch 2 Step 62600 Loss 2.9736\n",
            "Epoch 2 Step 62800 Loss 3.0200\n",
            "Epoch 2 Step 63000 Loss 2.9341\n",
            "Epoch 2 Step 63200 Loss 2.8252\n",
            "Epoch 2 Step 63400 Loss 2.7480\n",
            "Epoch 2 Step 63600 Loss 2.7522\n",
            "Epoch 2 Step 63800 Loss 2.6978\n",
            "Epoch 2 Step 64000 Loss 2.6512\n",
            "Epoch 2 Step 64200 Loss 2.4875\n",
            "Epoch 2 Step 64400 Loss 2.7080\n",
            "Epoch 2 Step 64600 Loss 2.5549\n",
            "Epoch 2 Step 64800 Loss 2.6885\n",
            "Epoch 2 Step 65000 Loss 2.7698\n",
            "Epoch 2 Step 65200 Loss 2.8599\n",
            "Epoch 2 Step 65400 Loss 2.8414\n",
            "Epoch 2 Step 65600 Loss 2.7134\n",
            "Epoch 2 Step 65800 Loss 2.8245\n",
            "Epoch 2 Step 66000 Loss 2.7360\n",
            "Epoch 2 Step 66200 Loss 3.1039\n",
            "Epoch 2 Step 66400 Loss 2.7728\n",
            "Epoch 2 Step 66600 Loss 2.9731\n",
            "Epoch 2 Step 66800 Loss 2.7699\n",
            "Epoch 2 Step 67000 Loss 2.7468\n",
            "Epoch 2 Step 67200 Loss 3.1184\n",
            "Epoch 2 Step 67400 Loss 2.5976\n",
            "Epoch 2 Step 67600 Loss 2.7413\n",
            "Epoch 2 Step 67800 Loss 2.6171\n",
            "Epoch 2 Step 68000 Loss 2.9012\n",
            "Epoch 2 Step 68200 Loss 2.9868\n",
            "Epoch 2 Step 68400 Loss 2.5530\n",
            "Epoch 2 Step 68600 Loss 2.6797\n",
            "Epoch 2 Step 68800 Loss 2.8166\n",
            "Epoch 2 Step 69000 Loss 3.0687\n",
            "Epoch 2 Step 69200 Loss 2.9064\n",
            "Epoch 2 Step 69400 Loss 2.8282\n",
            "Epoch 2 Step 69600 Loss 2.5305\n",
            "Epoch 2 Step 69800 Loss 2.7937\n",
            "Epoch 2 Step 70000 Loss 2.7879\n",
            "Epoch 2 Step 70200 Loss 2.6486\n",
            "Epoch 2 Step 70400 Loss 2.8587\n",
            "Epoch 2 Step 70600 Loss 3.0611\n",
            "Epoch 2 Step 70800 Loss 3.1301\n",
            "Epoch 2 Step 71000 Loss 2.8619\n",
            "Epoch 2 Step 71200 Loss 2.8150\n",
            "Epoch 2 Step 71400 Loss 2.8604\n",
            "Epoch 2 Step 71600 Loss 2.3512\n",
            "Epoch 2 Step 71800 Loss 2.9421\n",
            "Epoch 2 Step 72000 Loss 2.4757\n",
            "Epoch 2 Step 72200 Loss 2.7396\n",
            "Epoch 2 Step 72400 Loss 2.5859\n",
            "Epoch 2 Step 72600 Loss 2.6822\n",
            "Epoch 2 Step 72800 Loss 2.6377\n",
            "Epoch 2 Step 73000 Loss 2.8838\n",
            "Epoch 2 Step 73200 Loss 2.5304\n",
            "Epoch 2 Step 73400 Loss 2.6180\n",
            "Epoch 2 Step 73600 Loss 2.8648\n",
            "Epoch 2 Step 73800 Loss 2.6226\n",
            "Epoch 2 Step 74000 Loss 2.5969\n",
            "Epoch 2 Step 74200 Loss 2.8443\n",
            "Epoch 2 Step 74400 Loss 2.4412\n",
            "Epoch 2 Step 74600 Loss 3.1292\n",
            "Epoch 2 Step 74800 Loss 2.8974\n",
            "Epoch 2 Step 75000 Loss 2.4322\n",
            "Epoch 2 Step 75200 Loss 2.8811\n",
            "Epoch 2 Step 75400 Loss 2.9966\n",
            "Epoch 2 Step 75600 Loss 2.6135\n",
            "Epoch 2 Step 75800 Loss 2.6846\n",
            "Epoch 2 Step 76000 Loss 2.7243\n",
            "Epoch 2 Step 76200 Loss 2.5608\n",
            "Epoch 2 Step 76400 Loss 2.7112\n",
            "Epoch 2 Step 76600 Loss 2.8475\n",
            "Epoch 2 Step 76800 Loss 2.6534\n",
            "Epoch 2 Step 77000 Loss 2.7646\n",
            "Epoch 2 Step 77200 Loss 2.6799\n",
            "Epoch 2 Step 77400 Loss 2.8213\n",
            "Epoch 2 Step 77600 Loss 2.6747\n",
            "Epoch 2 Step 77800 Loss 2.8683\n",
            "Epoch 2 Step 78000 Loss 2.4971\n",
            "Epoch 2 Step 78200 Loss 2.6434\n",
            "Epoch 2 Step 78400 Loss 2.8892\n",
            "Epoch 2 Step 78600 Loss 2.4625\n",
            "Epoch 2 Step 78800 Loss 2.5891\n",
            "Epoch 2 Step 79000 Loss 2.5036\n",
            "Epoch 2 Step 79200 Loss 2.7993\n",
            "Epoch 2 Step 79400 Loss 2.8083\n",
            "Epoch 2 Step 79600 Loss 2.7422\n",
            "Epoch 2 Step 79800 Loss 2.5538\n",
            "Epoch 2 Step 80000 Loss 2.5592\n",
            "Epoch 2 Step 80200 Loss 2.6563\n",
            "Epoch 2 Step 80400 Loss 2.3597\n",
            "Epoch 2 Step 80600 Loss 2.8418\n",
            "Epoch 2 Step 80800 Loss 2.6158\n",
            "Epoch 2 Step 81000 Loss 2.2839\n",
            "Epoch 2 Step 81200 Loss 2.7021\n",
            "Epoch 2 Step 81400 Loss 2.5286\n",
            "Epoch 2 Step 81600 Loss 2.9073\n",
            "Epoch 2 Step 81800 Loss 2.5773\n",
            "Epoch 2 Step 82000 Loss 2.9635\n",
            "Epoch 2 Step 82200 Loss 2.5541\n",
            "Epoch 2 Step 82400 Loss 3.1140\n",
            "Epoch 2 Step 82600 Loss 2.6372\n",
            "Epoch 2 Step 82800 Loss 2.9251\n",
            "Epoch 2 Step 83000 Loss 2.5901\n",
            "Epoch 2 Step 83200 Loss 2.5865\n",
            "Epoch 2 Step 83400 Loss 2.6039\n",
            "Epoch 2 Step 83600 Loss 2.6246\n",
            "Epoch 2 Step 83800 Loss 2.5097\n",
            "Epoch 2 Step 84000 Loss 2.6524\n",
            "Epoch 2 Step 84200 Loss 2.8299\n",
            "Epoch 2 Step 84400 Loss 2.9219\n",
            "Epoch 2 Step 84600 Loss 2.5088\n",
            "Epoch 2 Step 84800 Loss 3.0015\n",
            "Epoch 2 Step 85000 Loss 2.8045\n",
            "Epoch 2 Step 85200 Loss 2.4946\n",
            "Epoch 2 Step 85400 Loss 2.7268\n",
            "Epoch 2 Step 85600 Loss 2.8760\n",
            "Epoch 2 Step 85800 Loss 2.7146\n",
            "Epoch 2 Step 86000 Loss 2.6186\n",
            "Epoch 2 Step 86200 Loss 2.8419\n",
            "Epoch 2 Step 86400 Loss 2.7166\n",
            "Epoch 2 Step 86600 Loss 2.5114\n",
            "Epoch 2 Step 86800 Loss 2.5571\n",
            "Epoch 2 Step 87000 Loss 2.8637\n",
            "Epoch 2 Step 87200 Loss 2.6456\n",
            "Epoch 2 Step 87400 Loss 2.9059\n",
            "Epoch 2 Step 87600 Loss 2.6440\n",
            "Epoch 2 Step 87800 Loss 2.8693\n",
            "Epoch 2 Step 88000 Loss 3.1311\n",
            "Epoch 2 Step 88200 Loss 3.0841\n",
            "Epoch 2 Step 88400 Loss 2.7277\n",
            "Epoch 2 Step 88600 Loss 2.7901\n",
            "Epoch 2 Step 88800 Loss 2.8150\n",
            "Epoch 2 Step 89000 Loss 2.7238\n",
            "Epoch 2 Step 89200 Loss 2.8311\n",
            "Epoch 2 Step 89400 Loss 2.7993\n",
            "Epoch 2 Step 89600 Loss 2.6931\n",
            "Epoch 2 Step 89800 Loss 3.0147\n",
            "Epoch 2 Step 90000 Loss 2.4994\n",
            "Epoch 2 Step 90200 Loss 2.5300\n",
            "Epoch 2 Step 90400 Loss 2.9122\n",
            "Epoch 2 Step 90600 Loss 2.4248\n",
            "Epoch 2 Step 90800 Loss 2.6169\n",
            "Epoch 2 Step 91000 Loss 2.3603\n",
            "Epoch 2 Step 91200 Loss 2.6565\n",
            "Epoch 2 Step 91400 Loss 2.6608\n",
            "Epoch 2 Step 91600 Loss 2.6241\n",
            "Epoch 2 finished. Last loss = 2.5685\n",
            "Epoch 3 Step 0 Loss 2.4923\n",
            "Epoch 3 Step 200 Loss 2.7197\n",
            "Epoch 3 Step 400 Loss 2.9627\n",
            "Epoch 3 Step 600 Loss 2.8627\n",
            "Epoch 3 Step 800 Loss 2.5934\n",
            "Epoch 3 Step 1000 Loss 2.3920\n",
            "Epoch 3 Step 1200 Loss 2.8324\n",
            "Epoch 3 Step 1400 Loss 3.2122\n",
            "Epoch 3 Step 1600 Loss 2.6703\n",
            "Epoch 3 Step 1800 Loss 2.6638\n",
            "Epoch 3 Step 2000 Loss 2.7590\n",
            "Epoch 3 Step 2200 Loss 2.6383\n",
            "Epoch 3 Step 2400 Loss 2.6032\n",
            "Epoch 3 Step 2600 Loss 2.7119\n",
            "Epoch 3 Step 2800 Loss 2.6714\n",
            "Epoch 3 Step 3000 Loss 2.8334\n",
            "Epoch 3 Step 3200 Loss 2.9115\n",
            "Epoch 3 Step 3400 Loss 2.6164\n",
            "Epoch 3 Step 3600 Loss 2.7336\n",
            "Epoch 3 Step 3800 Loss 2.5644\n",
            "Epoch 3 Step 4000 Loss 2.4332\n",
            "Epoch 3 Step 4200 Loss 2.3136\n",
            "Epoch 3 Step 4400 Loss 2.4437\n",
            "Epoch 3 Step 4600 Loss 2.7353\n",
            "Epoch 3 Step 4800 Loss 2.5900\n",
            "Epoch 3 Step 5000 Loss 2.5473\n",
            "Epoch 3 Step 5200 Loss 2.8949\n",
            "Epoch 3 Step 5400 Loss 2.9279\n",
            "Epoch 3 Step 5600 Loss 2.7311\n",
            "Epoch 3 Step 5800 Loss 2.5614\n",
            "Epoch 3 Step 6000 Loss 2.7265\n",
            "Epoch 3 Step 6200 Loss 2.4724\n",
            "Epoch 3 Step 6400 Loss 2.7334\n",
            "Epoch 3 Step 6600 Loss 2.6505\n",
            "Epoch 3 Step 6800 Loss 2.7864\n",
            "Epoch 3 Step 7000 Loss 2.6496\n",
            "Epoch 3 Step 7200 Loss 2.7176\n",
            "Epoch 3 Step 7400 Loss 2.4978\n",
            "Epoch 3 Step 7600 Loss 2.5713\n",
            "Epoch 3 Step 7800 Loss 2.4270\n",
            "Epoch 3 Step 8000 Loss 2.8943\n",
            "Epoch 3 Step 8200 Loss 2.9834\n",
            "Epoch 3 Step 8400 Loss 2.4280\n",
            "Epoch 3 Step 8600 Loss 2.7783\n",
            "Epoch 3 Step 8800 Loss 2.5515\n",
            "Epoch 3 Step 9000 Loss 2.4027\n",
            "Epoch 3 Step 9200 Loss 2.8121\n",
            "Epoch 3 Step 9400 Loss 2.5411\n",
            "Epoch 3 Step 9600 Loss 2.7712\n",
            "Epoch 3 Step 9800 Loss 2.4914\n",
            "Epoch 3 Step 10000 Loss 2.4675\n",
            "Epoch 3 Step 10200 Loss 2.2150\n",
            "Epoch 3 Step 10400 Loss 2.6129\n",
            "Epoch 3 Step 10600 Loss 2.7397\n",
            "Epoch 3 Step 10800 Loss 2.4022\n",
            "Epoch 3 Step 11000 Loss 2.5725\n",
            "Epoch 3 Step 11200 Loss 2.7883\n",
            "Epoch 3 Step 11400 Loss 2.4620\n",
            "Epoch 3 Step 11600 Loss 2.3891\n",
            "Epoch 3 Step 11800 Loss 2.7614\n",
            "Epoch 3 Step 12000 Loss 2.7427\n",
            "Epoch 3 Step 12200 Loss 2.6105\n",
            "Epoch 3 Step 12400 Loss 2.4399\n",
            "Epoch 3 Step 12600 Loss 2.5809\n",
            "Epoch 3 Step 12800 Loss 2.8857\n",
            "Epoch 3 Step 13000 Loss 2.7698\n",
            "Epoch 3 Step 13200 Loss 2.8490\n",
            "Epoch 3 Step 13400 Loss 2.6733\n",
            "Epoch 3 Step 13600 Loss 2.7065\n",
            "Epoch 3 Step 13800 Loss 2.3222\n",
            "Epoch 3 Step 14000 Loss 2.7064\n",
            "Epoch 3 Step 14200 Loss 2.4309\n",
            "Epoch 3 Step 14400 Loss 2.6372\n",
            "Epoch 3 Step 14600 Loss 2.4875\n",
            "Epoch 3 Step 14800 Loss 2.5076\n",
            "Epoch 3 Step 15000 Loss 2.6971\n",
            "Epoch 3 Step 15200 Loss 2.4874\n",
            "Epoch 3 Step 15400 Loss 2.5593\n",
            "Epoch 3 Step 15600 Loss 2.8169\n",
            "Epoch 3 Step 15800 Loss 2.9844\n",
            "Epoch 3 Step 16000 Loss 2.3496\n",
            "Epoch 3 Step 16200 Loss 2.5536\n",
            "Epoch 3 Step 16400 Loss 2.6098\n",
            "Epoch 3 Step 16600 Loss 2.8120\n",
            "Epoch 3 Step 16800 Loss 2.8585\n",
            "Epoch 3 Step 17000 Loss 3.0184\n",
            "Epoch 3 Step 17200 Loss 2.5147\n",
            "Epoch 3 Step 17400 Loss 2.4677\n",
            "Epoch 3 Step 17600 Loss 2.4776\n",
            "Epoch 3 Step 17800 Loss 2.5619\n",
            "Epoch 3 Step 18000 Loss 2.9117\n",
            "Epoch 3 Step 18200 Loss 2.8394\n",
            "Epoch 3 Step 18400 Loss 2.6025\n",
            "Epoch 3 Step 18600 Loss 2.5873\n",
            "Epoch 3 Step 18800 Loss 2.8883\n",
            "Epoch 3 Step 19000 Loss 2.5856\n",
            "Epoch 3 Step 19200 Loss 2.7823\n",
            "Epoch 3 Step 19400 Loss 2.2640\n",
            "Epoch 3 Step 19600 Loss 2.9328\n",
            "Epoch 3 Step 19800 Loss 2.2333\n",
            "Epoch 3 Step 20000 Loss 2.3324\n",
            "Epoch 3 Step 20200 Loss 2.6112\n",
            "Epoch 3 Step 20400 Loss 2.6466\n",
            "Epoch 3 Step 20600 Loss 2.7663\n",
            "Epoch 3 Step 20800 Loss 2.5389\n",
            "Epoch 3 Step 21000 Loss 2.3785\n",
            "Epoch 3 Step 21200 Loss 2.5597\n",
            "Epoch 3 Step 21400 Loss 2.5791\n",
            "Epoch 3 Step 21600 Loss 2.7172\n",
            "Epoch 3 Step 21800 Loss 3.0024\n",
            "Epoch 3 Step 22000 Loss 2.5831\n",
            "Epoch 3 Step 22200 Loss 2.5451\n",
            "Epoch 3 Step 22400 Loss 2.6111\n",
            "Epoch 3 Step 22600 Loss 2.6235\n",
            "Epoch 3 Step 22800 Loss 2.6461\n",
            "Epoch 3 Step 23000 Loss 2.6286\n",
            "Epoch 3 Step 23200 Loss 2.6011\n",
            "Epoch 3 Step 23400 Loss 2.8797\n",
            "Epoch 3 Step 23600 Loss 2.3889\n",
            "Epoch 3 Step 23800 Loss 2.5763\n",
            "Epoch 3 Step 24000 Loss 2.4784\n",
            "Epoch 3 Step 24200 Loss 2.3851\n",
            "Epoch 3 Step 24400 Loss 2.4342\n",
            "Epoch 3 Step 24600 Loss 2.5902\n",
            "Epoch 3 Step 24800 Loss 2.5474\n",
            "Epoch 3 Step 25000 Loss 2.7573\n",
            "Epoch 3 Step 25200 Loss 2.9544\n",
            "Epoch 3 Step 25400 Loss 2.3466\n",
            "Epoch 3 Step 25600 Loss 2.8437\n",
            "Epoch 3 Step 25800 Loss 2.7490\n",
            "Epoch 3 Step 26000 Loss 2.7901\n",
            "Epoch 3 Step 26200 Loss 2.7843\n",
            "Epoch 3 Step 26400 Loss 2.4609\n",
            "Epoch 3 Step 26600 Loss 2.5454\n",
            "Epoch 3 Step 26800 Loss 2.6391\n",
            "Epoch 3 Step 27000 Loss 2.5081\n",
            "Epoch 3 Step 27200 Loss 2.4946\n",
            "Epoch 3 Step 27400 Loss 2.7858\n",
            "Epoch 3 Step 27600 Loss 2.6125\n",
            "Epoch 3 Step 27800 Loss 2.3762\n",
            "Epoch 3 Step 28000 Loss 2.2657\n",
            "Epoch 3 Step 28200 Loss 2.7052\n",
            "Epoch 3 Step 28400 Loss 2.7791\n",
            "Epoch 3 Step 28600 Loss 2.4623\n",
            "Epoch 3 Step 28800 Loss 2.8944\n",
            "Epoch 3 Step 29000 Loss 2.6878\n",
            "Epoch 3 Step 29200 Loss 2.9075\n",
            "Epoch 3 Step 29400 Loss 2.4750\n",
            "Epoch 3 Step 29600 Loss 2.6969\n",
            "Epoch 3 Step 29800 Loss 2.5719\n",
            "Epoch 3 Step 30000 Loss 2.4420\n",
            "Epoch 3 Step 30200 Loss 2.5349\n",
            "Epoch 3 Step 30400 Loss 2.8073\n",
            "Epoch 3 Step 30600 Loss 2.6109\n",
            "Epoch 3 Step 30800 Loss 2.2663\n",
            "Epoch 3 Step 31000 Loss 2.4878\n",
            "Epoch 3 Step 31200 Loss 2.4594\n",
            "Epoch 3 Step 31400 Loss 2.2752\n",
            "Epoch 3 Step 31600 Loss 2.7959\n",
            "Epoch 3 Step 31800 Loss 2.2805\n",
            "Epoch 3 Step 32000 Loss 2.4338\n",
            "Epoch 3 Step 32200 Loss 2.3289\n",
            "Epoch 3 Step 32400 Loss 2.7726\n",
            "Epoch 3 Step 32600 Loss 2.7331\n",
            "Epoch 3 Step 32800 Loss 2.5316\n",
            "Epoch 3 Step 33000 Loss 2.5070\n",
            "Epoch 3 Step 33200 Loss 2.4465\n",
            "Epoch 3 Step 33400 Loss 2.6584\n",
            "Epoch 3 Step 33600 Loss 2.5380\n",
            "Epoch 3 Step 33800 Loss 2.6764\n",
            "Epoch 3 Step 34000 Loss 2.5644\n",
            "Epoch 3 Step 34200 Loss 2.6347\n",
            "Epoch 3 Step 34400 Loss 2.3400\n",
            "Epoch 3 Step 34600 Loss 2.7752\n",
            "Epoch 3 Step 34800 Loss 2.8022\n",
            "Epoch 3 Step 35000 Loss 2.4266\n",
            "Epoch 3 Step 35200 Loss 2.7678\n",
            "Epoch 3 Step 35400 Loss 2.6346\n",
            "Epoch 3 Step 35600 Loss 2.7875\n",
            "Epoch 3 Step 35800 Loss 2.5719\n",
            "Epoch 3 Step 36000 Loss 2.3803\n",
            "Epoch 3 Step 36200 Loss 2.3865\n",
            "Epoch 3 Step 36400 Loss 2.5599\n",
            "Epoch 3 Step 36600 Loss 2.4417\n",
            "Epoch 3 Step 36800 Loss 2.4669\n",
            "Epoch 3 Step 37000 Loss 2.6249\n",
            "Epoch 3 Step 37200 Loss 2.2923\n",
            "Epoch 3 Step 37400 Loss 2.4982\n",
            "Epoch 3 Step 37600 Loss 2.4695\n",
            "Epoch 3 Step 37800 Loss 2.8638\n",
            "Epoch 3 Step 38000 Loss 2.6146\n",
            "Epoch 3 Step 38200 Loss 2.5596\n",
            "Epoch 3 Step 38400 Loss 2.3460\n",
            "Epoch 3 Step 38600 Loss 2.5152\n",
            "Epoch 3 Step 38800 Loss 2.6895\n",
            "Epoch 3 Step 39000 Loss 2.4831\n",
            "Epoch 3 Step 39200 Loss 2.5054\n",
            "Epoch 3 Step 39400 Loss 2.4205\n",
            "Epoch 3 Step 39600 Loss 2.2789\n",
            "Epoch 3 Step 39800 Loss 2.7206\n",
            "Epoch 3 Step 40000 Loss 2.4026\n",
            "Epoch 3 Step 40200 Loss 2.7832\n",
            "Epoch 3 Step 40400 Loss 2.6305\n",
            "Epoch 3 Step 40600 Loss 2.6691\n",
            "Epoch 3 Step 40800 Loss 2.6514\n",
            "Epoch 3 Step 41000 Loss 2.2598\n",
            "Epoch 3 Step 41200 Loss 2.6544\n",
            "Epoch 3 Step 41400 Loss 2.4963\n",
            "Epoch 3 Step 41600 Loss 2.6855\n",
            "Epoch 3 Step 41800 Loss 2.3578\n",
            "Epoch 3 Step 42000 Loss 2.5631\n",
            "Epoch 3 Step 42200 Loss 2.4856\n",
            "Epoch 3 Step 42400 Loss 2.6606\n",
            "Epoch 3 Step 42600 Loss 2.6270\n",
            "Epoch 3 Step 42800 Loss 2.6935\n",
            "Epoch 3 Step 43000 Loss 2.4858\n",
            "Epoch 3 Step 43200 Loss 2.5004\n",
            "Epoch 3 Step 43400 Loss 2.6853\n",
            "Epoch 3 Step 43600 Loss 2.4976\n",
            "Epoch 3 Step 43800 Loss 2.5723\n",
            "Epoch 3 Step 44000 Loss 2.3951\n",
            "Epoch 3 Step 44200 Loss 2.3874\n",
            "Epoch 3 Step 44400 Loss 2.4113\n",
            "Epoch 3 Step 44600 Loss 3.0543\n",
            "Epoch 3 Step 44800 Loss 2.5766\n",
            "Epoch 3 Step 45000 Loss 2.7104\n",
            "Epoch 3 Step 45200 Loss 2.5151\n",
            "Epoch 3 Step 45400 Loss 2.5648\n",
            "Epoch 3 Step 45600 Loss 2.4843\n",
            "Epoch 3 Step 45800 Loss 2.6340\n",
            "Epoch 3 Step 46000 Loss 2.5336\n",
            "Epoch 3 Step 46200 Loss 2.7961\n",
            "Epoch 3 Step 46400 Loss 2.5671\n",
            "Epoch 3 Step 46600 Loss 2.7081\n",
            "Epoch 3 Step 46800 Loss 2.4563\n",
            "Epoch 3 Step 47000 Loss 2.6067\n",
            "Epoch 3 Step 47200 Loss 2.8267\n",
            "Epoch 3 Step 47400 Loss 2.8220\n",
            "Epoch 3 Step 47600 Loss 2.2898\n",
            "Epoch 3 Step 47800 Loss 2.4533\n",
            "Epoch 3 Step 48000 Loss 2.4860\n",
            "Epoch 3 Step 48200 Loss 2.7719\n",
            "Epoch 3 Step 48400 Loss 2.5310\n",
            "Epoch 3 Step 48600 Loss 2.6909\n",
            "Epoch 3 Step 48800 Loss 2.4317\n",
            "Epoch 3 Step 49000 Loss 2.7978\n",
            "Epoch 3 Step 49200 Loss 2.5229\n",
            "Epoch 3 Step 49400 Loss 2.5762\n",
            "Epoch 3 Step 49600 Loss 2.3796\n",
            "Epoch 3 Step 49800 Loss 2.6619\n",
            "Epoch 3 Step 50000 Loss 2.4229\n",
            "Epoch 3 Step 50200 Loss 2.8044\n",
            "Epoch 3 Step 50400 Loss 2.5645\n",
            "Epoch 3 Step 50600 Loss 2.4723\n",
            "Epoch 3 Step 50800 Loss 2.5375\n",
            "Epoch 3 Step 51000 Loss 2.6010\n",
            "Epoch 3 Step 51200 Loss 2.5221\n",
            "Epoch 3 Step 51400 Loss 2.5234\n",
            "Epoch 3 Step 51600 Loss 2.6060\n",
            "Epoch 3 Step 51800 Loss 2.6091\n",
            "Epoch 3 Step 52000 Loss 2.6006\n",
            "Epoch 3 Step 52200 Loss 2.6666\n",
            "Epoch 3 Step 52400 Loss 2.6814\n",
            "Epoch 3 Step 52600 Loss 2.3809\n",
            "Epoch 3 Step 52800 Loss 2.4925\n",
            "Epoch 3 Step 53000 Loss 2.4374\n",
            "Epoch 3 Step 53200 Loss 2.6608\n",
            "Epoch 3 Step 53400 Loss 2.4289\n",
            "Epoch 3 Step 53600 Loss 2.4652\n",
            "Epoch 3 Step 53800 Loss 2.4289\n",
            "Epoch 3 Step 54000 Loss 2.5429\n",
            "Epoch 3 Step 54200 Loss 2.8432\n",
            "Epoch 3 Step 54400 Loss 2.4533\n",
            "Epoch 3 Step 54600 Loss 2.7222\n",
            "Epoch 3 Step 54800 Loss 2.4893\n",
            "Epoch 3 Step 55000 Loss 2.6737\n",
            "Epoch 3 Step 55200 Loss 2.7304\n",
            "Epoch 3 Step 55400 Loss 2.7016\n",
            "Epoch 3 Step 55600 Loss 2.2758\n",
            "Epoch 3 Step 55800 Loss 2.6517\n",
            "Epoch 3 Step 56000 Loss 2.4851\n",
            "Epoch 3 Step 56200 Loss 2.5652\n",
            "Epoch 3 Step 56400 Loss 2.6741\n",
            "Epoch 3 Step 56600 Loss 2.4286\n",
            "Epoch 3 Step 56800 Loss 2.8746\n",
            "Epoch 3 Step 57000 Loss 2.7807\n",
            "Epoch 3 Step 57200 Loss 2.5917\n",
            "Epoch 3 Step 57400 Loss 2.9794\n",
            "Epoch 3 Step 57600 Loss 2.3131\n",
            "Epoch 3 Step 57800 Loss 2.6850\n",
            "Epoch 3 Step 58000 Loss 2.4537\n",
            "Epoch 3 Step 58200 Loss 2.6460\n",
            "Epoch 3 Step 58400 Loss 2.2258\n",
            "Epoch 3 Step 58600 Loss 2.6853\n",
            "Epoch 3 Step 58800 Loss 2.7080\n",
            "Epoch 3 Step 59000 Loss 2.7934\n",
            "Epoch 3 Step 59200 Loss 2.7187\n",
            "Epoch 3 Step 59400 Loss 2.6181\n",
            "Epoch 3 Step 59600 Loss 2.7402\n",
            "Epoch 3 Step 59800 Loss 2.5838\n",
            "Epoch 3 Step 60000 Loss 2.4283\n",
            "Epoch 3 Step 60200 Loss 2.4263\n",
            "Epoch 3 Step 60400 Loss 2.7642\n",
            "Epoch 3 Step 60600 Loss 2.5710\n",
            "Epoch 3 Step 60800 Loss 2.6474\n",
            "Epoch 3 Step 61000 Loss 2.7708\n",
            "Epoch 3 Step 61200 Loss 2.6015\n",
            "Epoch 3 Step 61400 Loss 2.5232\n",
            "Epoch 3 Step 61600 Loss 2.6021\n",
            "Epoch 3 Step 61800 Loss 2.3807\n",
            "Epoch 3 Step 62000 Loss 2.6179\n",
            "Epoch 3 Step 62200 Loss 2.5011\n",
            "Epoch 3 Step 62400 Loss 2.8485\n",
            "Epoch 3 Step 62600 Loss 2.4894\n",
            "Epoch 3 Step 62800 Loss 2.7325\n",
            "Epoch 3 Step 63000 Loss 2.6069\n",
            "Epoch 3 Step 63200 Loss 2.6626\n",
            "Epoch 3 Step 63400 Loss 2.4734\n",
            "Epoch 3 Step 63600 Loss 2.6029\n",
            "Epoch 3 Step 63800 Loss 2.5351\n",
            "Epoch 3 Step 64000 Loss 2.2508\n",
            "Epoch 3 Step 64200 Loss 2.8544\n",
            "Epoch 3 Step 64400 Loss 2.5991\n",
            "Epoch 3 Step 64600 Loss 2.3308\n",
            "Epoch 3 Step 64800 Loss 2.4862\n",
            "Epoch 3 Step 65000 Loss 2.6727\n",
            "Epoch 3 Step 65200 Loss 2.3729\n",
            "Epoch 3 Step 65400 Loss 2.7794\n",
            "Epoch 3 Step 65600 Loss 2.4543\n",
            "Epoch 3 Step 65800 Loss 2.3085\n",
            "Epoch 3 Step 66000 Loss 2.5859\n",
            "Epoch 3 Step 66200 Loss 2.4687\n",
            "Epoch 3 Step 66400 Loss 2.6392\n",
            "Epoch 3 Step 66600 Loss 2.6344\n",
            "Epoch 3 Step 66800 Loss 2.6086\n",
            "Epoch 3 Step 67000 Loss 2.6378\n",
            "Epoch 3 Step 67200 Loss 2.7197\n",
            "Epoch 3 Step 67400 Loss 2.5193\n",
            "Epoch 3 Step 67600 Loss 2.5873\n",
            "Epoch 3 Step 67800 Loss 2.5085\n",
            "Epoch 3 Step 68000 Loss 2.7127\n",
            "Epoch 3 Step 68200 Loss 2.5071\n",
            "Epoch 3 Step 68400 Loss 2.4559\n",
            "Epoch 3 Step 68600 Loss 2.5486\n",
            "Epoch 3 Step 68800 Loss 2.4174\n",
            "Epoch 3 Step 69000 Loss 2.6055\n",
            "Epoch 3 Step 69200 Loss 2.5361\n",
            "Epoch 3 Step 69400 Loss 2.3929\n",
            "Epoch 3 Step 69600 Loss 2.2062\n",
            "Epoch 3 Step 69800 Loss 2.8751\n",
            "Epoch 3 Step 70000 Loss 2.3195\n",
            "Epoch 3 Step 70200 Loss 2.4905\n",
            "Epoch 3 Step 70400 Loss 2.4375\n",
            "Epoch 3 Step 70600 Loss 2.5453\n",
            "Epoch 3 Step 70800 Loss 2.6527\n",
            "Epoch 3 Step 71000 Loss 2.3911\n",
            "Epoch 3 Step 71200 Loss 2.5359\n",
            "Epoch 3 Step 71400 Loss 2.5321\n",
            "Epoch 3 Step 71600 Loss 2.1530\n",
            "Epoch 3 Step 71800 Loss 2.6214\n",
            "Epoch 3 Step 72000 Loss 2.5083\n",
            "Epoch 3 Step 72200 Loss 2.5931\n",
            "Epoch 3 Step 72400 Loss 2.4588\n",
            "Epoch 3 Step 72600 Loss 2.4015\n",
            "Epoch 3 Step 72800 Loss 2.2024\n",
            "Epoch 3 Step 73000 Loss 2.7511\n",
            "Epoch 3 Step 73200 Loss 2.2496\n",
            "Epoch 3 Step 73400 Loss 2.3795\n",
            "Epoch 3 Step 73600 Loss 2.4159\n",
            "Epoch 3 Step 73800 Loss 2.7606\n",
            "Epoch 3 Step 74000 Loss 2.5957\n",
            "Epoch 3 Step 74200 Loss 2.6212\n",
            "Epoch 3 Step 74400 Loss 2.6911\n",
            "Epoch 3 Step 74600 Loss 2.4054\n",
            "Epoch 3 Step 74800 Loss 2.3177\n",
            "Epoch 3 Step 75000 Loss 2.3540\n",
            "Epoch 3 Step 75200 Loss 2.6359\n",
            "Epoch 3 Step 75400 Loss 2.5510\n",
            "Epoch 3 Step 75600 Loss 2.5289\n",
            "Epoch 3 Step 75800 Loss 2.5590\n",
            "Epoch 3 Step 76000 Loss 2.4901\n",
            "Epoch 3 Step 76200 Loss 2.3906\n",
            "Epoch 3 Step 76400 Loss 2.6514\n",
            "Epoch 3 Step 76600 Loss 2.4401\n",
            "Epoch 3 Step 76800 Loss 2.4283\n",
            "Epoch 3 Step 77000 Loss 2.5518\n",
            "Epoch 3 Step 77200 Loss 2.4599\n",
            "Epoch 3 Step 77400 Loss 2.5492\n",
            "Epoch 3 Step 77600 Loss 2.4611\n",
            "Epoch 3 Step 77800 Loss 2.3538\n",
            "Epoch 3 Step 78000 Loss 2.6987\n",
            "Epoch 3 Step 78200 Loss 2.4508\n",
            "Epoch 3 Step 78400 Loss 2.4877\n",
            "Epoch 3 Step 78600 Loss 2.3323\n",
            "Epoch 3 Step 78800 Loss 2.4394\n",
            "Epoch 3 Step 79000 Loss 2.4472\n",
            "Epoch 3 Step 79200 Loss 2.6285\n",
            "Epoch 3 Step 79400 Loss 2.7699\n",
            "Epoch 3 Step 79600 Loss 2.2405\n",
            "Epoch 3 Step 79800 Loss 2.8576\n",
            "Epoch 3 Step 80000 Loss 2.4180\n",
            "Epoch 3 Step 80200 Loss 2.5623\n",
            "Epoch 3 Step 80400 Loss 2.5976\n",
            "Epoch 3 Step 80600 Loss 2.4306\n",
            "Epoch 3 Step 80800 Loss 2.1018\n",
            "Epoch 3 Step 81000 Loss 2.7096\n",
            "Epoch 3 Step 81200 Loss 2.5866\n",
            "Epoch 3 Step 81400 Loss 2.5500\n",
            "Epoch 3 Step 81600 Loss 2.5301\n",
            "Epoch 3 Step 81800 Loss 2.0686\n",
            "Epoch 3 Step 82000 Loss 2.4070\n",
            "Epoch 3 Step 82200 Loss 2.2879\n",
            "Epoch 3 Step 82400 Loss 2.5994\n",
            "Epoch 3 Step 82600 Loss 2.6338\n",
            "Epoch 3 Step 82800 Loss 2.3126\n",
            "Epoch 3 Step 83000 Loss 2.6789\n",
            "Epoch 3 Step 83200 Loss 2.3467\n",
            "Epoch 3 Step 83400 Loss 2.4774\n",
            "Epoch 3 Step 83600 Loss 2.4065\n",
            "Epoch 3 Step 83800 Loss 2.5508\n",
            "Epoch 3 Step 84000 Loss 2.5320\n",
            "Epoch 3 Step 84200 Loss 2.3877\n",
            "Epoch 3 Step 84400 Loss 2.4755\n",
            "Epoch 3 Step 84600 Loss 2.3340\n",
            "Epoch 3 Step 84800 Loss 2.3775\n",
            "Epoch 3 Step 85000 Loss 2.2579\n",
            "Epoch 3 Step 85200 Loss 2.6978\n",
            "Epoch 3 Step 85400 Loss 2.3621\n",
            "Epoch 3 Step 85600 Loss 2.6409\n",
            "Epoch 3 Step 85800 Loss 2.5253\n",
            "Epoch 3 Step 86000 Loss 2.5410\n",
            "Epoch 3 Step 86200 Loss 2.5263\n",
            "Epoch 3 Step 86400 Loss 2.3718\n",
            "Epoch 3 Step 86600 Loss 2.6051\n",
            "Epoch 3 Step 86800 Loss 2.4247\n",
            "Epoch 3 Step 87000 Loss 2.0657\n",
            "Epoch 3 Step 87200 Loss 2.4172\n",
            "Epoch 3 Step 87400 Loss 2.7761\n",
            "Epoch 3 Step 87600 Loss 2.2975\n",
            "Epoch 3 Step 87800 Loss 2.0574\n",
            "Epoch 3 Step 88000 Loss 2.5655\n",
            "Epoch 3 Step 88200 Loss 2.5345\n",
            "Epoch 3 Step 88400 Loss 2.3679\n",
            "Epoch 3 Step 88600 Loss 2.5531\n",
            "Epoch 3 Step 88800 Loss 2.5216\n",
            "Epoch 3 Step 89000 Loss 2.5671\n",
            "Epoch 3 Step 89200 Loss 2.5024\n",
            "Epoch 3 Step 89400 Loss 2.8278\n",
            "Epoch 3 Step 89600 Loss 2.5303\n",
            "Epoch 3 Step 89800 Loss 2.3860\n",
            "Epoch 3 Step 90000 Loss 2.4396\n",
            "Epoch 3 Step 90200 Loss 2.4347\n",
            "Epoch 3 Step 90400 Loss 2.2976\n",
            "Epoch 3 Step 90600 Loss 2.4249\n",
            "Epoch 3 Step 90800 Loss 2.6537\n",
            "Epoch 3 Step 91000 Loss 2.6061\n",
            "Epoch 3 Step 91200 Loss 2.3521\n",
            "Epoch 3 Step 91400 Loss 2.4205\n",
            "Epoch 3 Step 91600 Loss 2.5187\n",
            "Epoch 3 finished. Last loss = 2.4813\n",
            "Epoch 4 Step 0 Loss 2.6782\n",
            "Epoch 4 Step 200 Loss 2.2375\n",
            "Epoch 4 Step 400 Loss 2.7272\n",
            "Epoch 4 Step 600 Loss 2.8179\n",
            "Epoch 4 Step 800 Loss 2.5554\n",
            "Epoch 4 Step 1000 Loss 2.2632\n",
            "Epoch 4 Step 1200 Loss 2.4111\n",
            "Epoch 4 Step 1400 Loss 2.4474\n",
            "Epoch 4 Step 1600 Loss 2.3564\n",
            "Epoch 4 Step 1800 Loss 2.5321\n",
            "Epoch 4 Step 2000 Loss 2.8908\n",
            "Epoch 4 Step 2200 Loss 2.5557\n",
            "Epoch 4 Step 2400 Loss 2.3651\n",
            "Epoch 4 Step 2600 Loss 2.3946\n",
            "Epoch 4 Step 2800 Loss 2.4447\n",
            "Epoch 4 Step 3000 Loss 2.4877\n",
            "Epoch 4 Step 3200 Loss 2.7629\n",
            "Epoch 4 Step 3400 Loss 2.2012\n",
            "Epoch 4 Step 3600 Loss 2.5783\n",
            "Epoch 4 Step 3800 Loss 2.4844\n",
            "Epoch 4 Step 4000 Loss 2.4514\n",
            "Epoch 4 Step 4200 Loss 2.4364\n",
            "Epoch 4 Step 4400 Loss 2.5240\n",
            "Epoch 4 Step 4600 Loss 2.4760\n",
            "Epoch 4 Step 4800 Loss 2.3033\n",
            "Epoch 4 Step 5000 Loss 2.3911\n",
            "Epoch 4 Step 5200 Loss 2.5321\n",
            "Epoch 4 Step 5400 Loss 2.3400\n",
            "Epoch 4 Step 5600 Loss 2.3750\n",
            "Epoch 4 Step 5800 Loss 2.2525\n",
            "Epoch 4 Step 6000 Loss 2.6010\n",
            "Epoch 4 Step 6200 Loss 2.4967\n",
            "Epoch 4 Step 6400 Loss 2.2630\n",
            "Epoch 4 Step 6600 Loss 2.3539\n",
            "Epoch 4 Step 6800 Loss 2.5381\n",
            "Epoch 4 Step 7000 Loss 2.4056\n",
            "Epoch 4 Step 7200 Loss 2.3125\n",
            "Epoch 4 Step 7400 Loss 2.5054\n",
            "Epoch 4 Step 7600 Loss 2.4274\n",
            "Epoch 4 Step 7800 Loss 2.1990\n",
            "Epoch 4 Step 8000 Loss 2.5103\n",
            "Epoch 4 Step 8200 Loss 2.3832\n",
            "Epoch 4 Step 8400 Loss 2.5871\n",
            "Epoch 4 Step 8600 Loss 2.3506\n",
            "Epoch 4 Step 8800 Loss 2.4125\n",
            "Epoch 4 Step 9000 Loss 2.6091\n",
            "Epoch 4 Step 9200 Loss 2.6010\n",
            "Epoch 4 Step 9400 Loss 1.9851\n",
            "Epoch 4 Step 9600 Loss 2.0998\n",
            "Epoch 4 Step 9800 Loss 2.5256\n",
            "Epoch 4 Step 10000 Loss 2.1693\n",
            "Epoch 4 Step 10200 Loss 2.5162\n",
            "Epoch 4 Step 10400 Loss 2.5727\n",
            "Epoch 4 Step 10600 Loss 2.6108\n",
            "Epoch 4 Step 10800 Loss 2.3704\n",
            "Epoch 4 Step 11000 Loss 2.4126\n",
            "Epoch 4 Step 11200 Loss 2.5689\n",
            "Epoch 4 Step 11400 Loss 2.5089\n",
            "Epoch 4 Step 11600 Loss 2.2320\n",
            "Epoch 4 Step 11800 Loss 2.4467\n",
            "Epoch 4 Step 12000 Loss 2.4817\n",
            "Epoch 4 Step 12200 Loss 2.4299\n",
            "Epoch 4 Step 12400 Loss 2.4903\n",
            "Epoch 4 Step 12600 Loss 2.6942\n",
            "Epoch 4 Step 12800 Loss 2.3683\n",
            "Epoch 4 Step 13000 Loss 2.5035\n",
            "Epoch 4 Step 13200 Loss 2.3928\n",
            "Epoch 4 Step 13400 Loss 2.6351\n",
            "Epoch 4 Step 13600 Loss 2.3865\n",
            "Epoch 4 Step 13800 Loss 2.3593\n",
            "Epoch 4 Step 14000 Loss 2.3639\n",
            "Epoch 4 Step 14200 Loss 2.3839\n",
            "Epoch 4 Step 14400 Loss 2.5204\n",
            "Epoch 4 Step 14600 Loss 2.4830\n",
            "Epoch 4 Step 14800 Loss 2.4016\n",
            "Epoch 4 Step 15000 Loss 2.2253\n",
            "Epoch 4 Step 15200 Loss 2.3156\n",
            "Epoch 4 Step 15400 Loss 2.4556\n",
            "Epoch 4 Step 15600 Loss 2.5412\n",
            "Epoch 4 Step 15800 Loss 2.6290\n",
            "Epoch 4 Step 16000 Loss 2.3543\n",
            "Epoch 4 Step 16200 Loss 2.5548\n",
            "Epoch 4 Step 16400 Loss 2.3672\n",
            "Epoch 4 Step 16600 Loss 1.9681\n",
            "Epoch 4 Step 16800 Loss 2.4764\n",
            "Epoch 4 Step 17000 Loss 2.3410\n",
            "Epoch 4 Step 17200 Loss 2.4731\n",
            "Epoch 4 Step 17400 Loss 2.3509\n",
            "Epoch 4 Step 17600 Loss 2.5016\n",
            "Epoch 4 Step 17800 Loss 2.3969\n",
            "Epoch 4 Step 18000 Loss 2.5686\n",
            "Epoch 4 Step 18200 Loss 2.6203\n",
            "Epoch 4 Step 18400 Loss 2.4330\n",
            "Epoch 4 Step 18600 Loss 2.4366\n",
            "Epoch 4 Step 18800 Loss 2.5881\n",
            "Epoch 4 Step 19000 Loss 2.4413\n",
            "Epoch 4 Step 19200 Loss 2.4667\n",
            "Epoch 4 Step 19400 Loss 2.6728\n",
            "Epoch 4 Step 19600 Loss 2.2329\n",
            "Epoch 4 Step 19800 Loss 2.4383\n",
            "Epoch 4 Step 20000 Loss 2.1158\n",
            "Epoch 4 Step 20200 Loss 2.5453\n",
            "Epoch 4 Step 20400 Loss 2.5951\n",
            "Epoch 4 Step 20600 Loss 2.3162\n",
            "Epoch 4 Step 20800 Loss 2.4192\n",
            "Epoch 4 Step 21000 Loss 2.4552\n",
            "Epoch 4 Step 21200 Loss 2.4242\n",
            "Epoch 4 Step 21400 Loss 2.2717\n",
            "Epoch 4 Step 21600 Loss 2.5662\n",
            "Epoch 4 Step 21800 Loss 2.7327\n",
            "Epoch 4 Step 22000 Loss 2.5273\n",
            "Epoch 4 Step 22200 Loss 2.3525\n",
            "Epoch 4 Step 22400 Loss 2.5132\n",
            "Epoch 4 Step 22600 Loss 2.2404\n",
            "Epoch 4 Step 22800 Loss 2.3931\n",
            "Epoch 4 Step 23000 Loss 2.2388\n",
            "Epoch 4 Step 23200 Loss 2.7666\n",
            "Epoch 4 Step 23400 Loss 2.5674\n",
            "Epoch 4 Step 23600 Loss 2.5224\n",
            "Epoch 4 Step 23800 Loss 2.3630\n",
            "Epoch 4 Step 24000 Loss 2.4335\n",
            "Epoch 4 Step 24200 Loss 2.0109\n",
            "Epoch 4 Step 24400 Loss 2.3471\n",
            "Epoch 4 Step 24600 Loss 2.3546\n",
            "Epoch 4 Step 24800 Loss 2.4031\n",
            "Epoch 4 Step 25000 Loss 2.3890\n",
            "Epoch 4 Step 25200 Loss 2.4728\n",
            "Epoch 4 Step 25400 Loss 2.4684\n",
            "Epoch 4 Step 25600 Loss 2.6797\n",
            "Epoch 4 Step 25800 Loss 2.3539\n",
            "Epoch 4 Step 26000 Loss 2.3872\n",
            "Epoch 4 Step 26200 Loss 2.3455\n",
            "Epoch 4 Step 26400 Loss 2.2959\n",
            "Epoch 4 Step 26600 Loss 2.5171\n",
            "Epoch 4 Step 26800 Loss 2.6599\n",
            "Epoch 4 Step 27000 Loss 2.4264\n",
            "Epoch 4 Step 27200 Loss 2.5160\n",
            "Epoch 4 Step 27400 Loss 2.5205\n",
            "Epoch 4 Step 27600 Loss 2.4160\n",
            "Epoch 4 Step 27800 Loss 2.2419\n",
            "Epoch 4 Step 28000 Loss 2.3119\n",
            "Epoch 4 Step 28200 Loss 2.3134\n",
            "Epoch 4 Step 28400 Loss 2.5358\n",
            "Epoch 4 Step 28600 Loss 2.3305\n",
            "Epoch 4 Step 28800 Loss 2.3458\n",
            "Epoch 4 Step 29000 Loss 2.3686\n",
            "Epoch 4 Step 29200 Loss 2.3198\n",
            "Epoch 4 Step 29400 Loss 2.4181\n",
            "Epoch 4 Step 29600 Loss 2.4391\n",
            "Epoch 4 Step 29800 Loss 2.1535\n",
            "Epoch 4 Step 30000 Loss 2.3357\n",
            "Epoch 4 Step 30200 Loss 2.3910\n",
            "Epoch 4 Step 30400 Loss 2.4491\n",
            "Epoch 4 Step 30600 Loss 2.2861\n",
            "Epoch 4 Step 30800 Loss 2.4367\n",
            "Epoch 4 Step 31000 Loss 2.5864\n",
            "Epoch 4 Step 31200 Loss 2.4873\n",
            "Epoch 4 Step 31400 Loss 2.4034\n",
            "Epoch 4 Step 31600 Loss 2.2855\n",
            "Epoch 4 Step 31800 Loss 2.0061\n",
            "Epoch 4 Step 32000 Loss 2.3443\n",
            "Epoch 4 Step 32200 Loss 2.1870\n",
            "Epoch 4 Step 32400 Loss 2.3200\n",
            "Epoch 4 Step 32600 Loss 2.5193\n",
            "Epoch 4 Step 32800 Loss 2.5738\n",
            "Epoch 4 Step 33000 Loss 2.3409\n",
            "Epoch 4 Step 33200 Loss 2.2630\n",
            "Epoch 4 Step 33400 Loss 2.6416\n",
            "Epoch 4 Step 33600 Loss 2.6246\n",
            "Epoch 4 Step 33800 Loss 2.3249\n",
            "Epoch 4 Step 34000 Loss 2.4999\n",
            "Epoch 4 Step 34200 Loss 2.5175\n",
            "Epoch 4 Step 34400 Loss 2.5790\n",
            "Epoch 4 Step 34600 Loss 2.1799\n",
            "Epoch 4 Step 34800 Loss 2.7046\n",
            "Epoch 4 Step 35000 Loss 2.6111\n",
            "Epoch 4 Step 35200 Loss 2.5895\n",
            "Epoch 4 Step 35400 Loss 2.6328\n",
            "Epoch 4 Step 35600 Loss 2.3421\n",
            "Epoch 4 Step 35800 Loss 2.4594\n",
            "Epoch 4 Step 36000 Loss 2.1984\n",
            "Epoch 4 Step 36200 Loss 2.2732\n",
            "Epoch 4 Step 36400 Loss 2.4630\n",
            "Epoch 4 Step 36600 Loss 2.1934\n",
            "Epoch 4 Step 36800 Loss 2.4748\n",
            "Epoch 4 Step 37000 Loss 2.7860\n",
            "Epoch 4 Step 37200 Loss 2.5603\n",
            "Epoch 4 Step 37400 Loss 2.2254\n",
            "Epoch 4 Step 37600 Loss 2.2208\n",
            "Epoch 4 Step 37800 Loss 2.3211\n",
            "Epoch 4 Step 38000 Loss 2.3777\n",
            "Epoch 4 Step 38200 Loss 2.7622\n",
            "Epoch 4 Step 38400 Loss 2.4985\n",
            "Epoch 4 Step 38600 Loss 2.5177\n",
            "Epoch 4 Step 38800 Loss 2.5775\n",
            "Epoch 4 Step 39000 Loss 2.6151\n",
            "Epoch 4 Step 39200 Loss 2.6948\n",
            "Epoch 4 Step 39400 Loss 2.6812\n",
            "Epoch 4 Step 39600 Loss 2.4814\n",
            "Epoch 4 Step 39800 Loss 2.4682\n",
            "Epoch 4 Step 40000 Loss 2.7254\n",
            "Epoch 4 Step 40200 Loss 2.5122\n",
            "Epoch 4 Step 40400 Loss 2.7308\n",
            "Epoch 4 Step 40600 Loss 2.5631\n",
            "Epoch 4 Step 40800 Loss 2.4765\n",
            "Epoch 4 Step 41000 Loss 2.5578\n",
            "Epoch 4 Step 41200 Loss 2.5102\n",
            "Epoch 4 Step 41400 Loss 2.3163\n",
            "Epoch 4 Step 41600 Loss 2.3555\n",
            "Epoch 4 Step 41800 Loss 2.6503\n",
            "Epoch 4 Step 42000 Loss 2.7514\n",
            "Epoch 4 Step 42200 Loss 2.2985\n",
            "Epoch 4 Step 42400 Loss 2.3070\n",
            "Epoch 4 Step 42600 Loss 2.4470\n",
            "Epoch 4 Step 42800 Loss 2.1216\n",
            "Epoch 4 Step 43000 Loss 2.5073\n",
            "Epoch 4 Step 43200 Loss 2.2867\n",
            "Epoch 4 Step 43400 Loss 2.5369\n",
            "Epoch 4 Step 43600 Loss 2.6666\n",
            "Epoch 4 Step 43800 Loss 2.2970\n",
            "Epoch 4 Step 44000 Loss 2.2129\n",
            "Epoch 4 Step 44200 Loss 2.5398\n",
            "Epoch 4 Step 44400 Loss 2.4000\n",
            "Epoch 4 Step 44600 Loss 2.3973\n",
            "Epoch 4 Step 44800 Loss 2.4771\n",
            "Epoch 4 Step 45000 Loss 2.4271\n",
            "Epoch 4 Step 45200 Loss 2.6267\n",
            "Epoch 4 Step 45400 Loss 2.6679\n",
            "Epoch 4 Step 45600 Loss 2.4498\n",
            "Epoch 4 Step 45800 Loss 2.4753\n",
            "Epoch 4 Step 46000 Loss 2.4425\n",
            "Epoch 4 Step 46200 Loss 2.4398\n",
            "Epoch 4 Step 46400 Loss 2.5033\n",
            "Epoch 4 Step 46600 Loss 2.2630\n",
            "Epoch 4 Step 46800 Loss 2.3559\n",
            "Epoch 4 Step 47000 Loss 2.3982\n",
            "Epoch 4 Step 47200 Loss 2.1972\n",
            "Epoch 4 Step 47400 Loss 2.1297\n",
            "Epoch 4 Step 47600 Loss 2.4762\n",
            "Epoch 4 Step 47800 Loss 2.3308\n",
            "Epoch 4 Step 48000 Loss 2.4159\n",
            "Epoch 4 Step 48200 Loss 2.4286\n",
            "Epoch 4 Step 48400 Loss 2.4826\n",
            "Epoch 4 Step 48600 Loss 2.2835\n",
            "Epoch 4 Step 48800 Loss 2.1940\n",
            "Epoch 4 Step 49000 Loss 2.2088\n",
            "Epoch 4 Step 49200 Loss 2.6396\n",
            "Epoch 4 Step 49400 Loss 2.2888\n",
            "Epoch 4 Step 49600 Loss 2.2376\n",
            "Epoch 4 Step 49800 Loss 2.4753\n",
            "Epoch 4 Step 50000 Loss 2.2790\n",
            "Epoch 4 Step 50200 Loss 2.3297\n",
            "Epoch 4 Step 50400 Loss 2.3883\n",
            "Epoch 4 Step 50600 Loss 2.6625\n",
            "Epoch 4 Step 50800 Loss 2.5876\n",
            "Epoch 4 Step 51000 Loss 2.5172\n",
            "Epoch 4 Step 51200 Loss 2.4374\n",
            "Epoch 4 Step 51400 Loss 2.6937\n",
            "Epoch 4 Step 51600 Loss 2.0486\n",
            "Epoch 4 Step 51800 Loss 2.8026\n",
            "Epoch 4 Step 52000 Loss 2.3808\n",
            "Epoch 4 Step 52200 Loss 2.3683\n",
            "Epoch 4 Step 52400 Loss 2.6127\n",
            "Epoch 4 Step 52600 Loss 2.2568\n",
            "Epoch 4 Step 52800 Loss 2.5042\n",
            "Epoch 4 Step 53000 Loss 2.3297\n",
            "Epoch 4 Step 53200 Loss 2.4802\n",
            "Epoch 4 Step 53400 Loss 2.4913\n",
            "Epoch 4 Step 53600 Loss 2.6699\n",
            "Epoch 4 Step 53800 Loss 2.5323\n",
            "Epoch 4 Step 54000 Loss 2.2932\n",
            "Epoch 4 Step 54200 Loss 2.4176\n",
            "Epoch 4 Step 54400 Loss 2.5957\n",
            "Epoch 4 Step 54600 Loss 2.3901\n",
            "Epoch 4 Step 54800 Loss 2.1948\n",
            "Epoch 4 Step 55000 Loss 2.6852\n",
            "Epoch 4 Step 55200 Loss 2.5782\n",
            "Epoch 4 Step 55400 Loss 2.4663\n",
            "Epoch 4 Step 55600 Loss 2.3044\n",
            "Epoch 4 Step 55800 Loss 2.3387\n",
            "Epoch 4 Step 56000 Loss 2.3545\n",
            "Epoch 4 Step 56200 Loss 2.4088\n",
            "Epoch 4 Step 56400 Loss 2.2984\n",
            "Epoch 4 Step 56600 Loss 2.5263\n",
            "Epoch 4 Step 56800 Loss 2.6867\n",
            "Epoch 4 Step 57000 Loss 2.3438\n",
            "Epoch 4 Step 57200 Loss 2.6668\n",
            "Epoch 4 Step 57400 Loss 2.4316\n",
            "Epoch 4 Step 57600 Loss 2.6023\n",
            "Epoch 4 Step 57800 Loss 2.3611\n",
            "Epoch 4 Step 58000 Loss 2.3248\n",
            "Epoch 4 Step 58200 Loss 2.0962\n",
            "Epoch 4 Step 58400 Loss 2.2297\n",
            "Epoch 4 Step 58600 Loss 2.3995\n",
            "Epoch 4 Step 58800 Loss 2.5479\n",
            "Epoch 4 Step 59000 Loss 2.5889\n",
            "Epoch 4 Step 59200 Loss 2.3951\n",
            "Epoch 4 Step 59400 Loss 2.2911\n",
            "Epoch 4 Step 59600 Loss 2.5328\n",
            "Epoch 4 Step 59800 Loss 2.3586\n",
            "Epoch 4 Step 60000 Loss 2.1360\n",
            "Epoch 4 Step 60200 Loss 2.4412\n",
            "Epoch 4 Step 60400 Loss 2.5105\n",
            "Epoch 4 Step 60600 Loss 2.3016\n",
            "Epoch 4 Step 60800 Loss 2.4703\n",
            "Epoch 4 Step 61000 Loss 2.3713\n",
            "Epoch 4 Step 61200 Loss 2.5935\n",
            "Epoch 4 Step 61400 Loss 2.5021\n",
            "Epoch 4 Step 61600 Loss 2.7438\n",
            "Epoch 4 Step 61800 Loss 2.4994\n",
            "Epoch 4 Step 62000 Loss 2.4351\n",
            "Epoch 4 Step 62200 Loss 2.6253\n",
            "Epoch 4 Step 62400 Loss 2.5023\n",
            "Epoch 4 Step 62600 Loss 2.6581\n",
            "Epoch 4 Step 62800 Loss 2.6480\n",
            "Epoch 4 Step 63000 Loss 2.4357\n",
            "Epoch 4 Step 63200 Loss 2.6364\n",
            "Epoch 4 Step 63400 Loss 2.3087\n",
            "Epoch 4 Step 63600 Loss 2.5762\n",
            "Epoch 4 Step 63800 Loss 2.4582\n",
            "Epoch 4 Step 64000 Loss 2.4651\n",
            "Epoch 4 Step 64200 Loss 2.3421\n",
            "Epoch 4 Step 64400 Loss 2.1820\n",
            "Epoch 4 Step 64600 Loss 2.2684\n",
            "Epoch 4 Step 64800 Loss 2.6201\n",
            "Epoch 4 Step 65000 Loss 2.6586\n",
            "Epoch 4 Step 65200 Loss 2.6138\n",
            "Epoch 4 Step 65400 Loss 2.6355\n",
            "Epoch 4 Step 65600 Loss 2.3249\n",
            "Epoch 4 Step 65800 Loss 2.5304\n",
            "Epoch 4 Step 66000 Loss 2.5968\n",
            "Epoch 4 Step 66200 Loss 2.3265\n",
            "Epoch 4 Step 66400 Loss 2.2833\n",
            "Epoch 4 Step 66600 Loss 2.2036\n",
            "Epoch 4 Step 66800 Loss 2.3551\n",
            "Epoch 4 Step 67000 Loss 2.7732\n",
            "Epoch 4 Step 67200 Loss 2.4381\n",
            "Epoch 4 Step 67400 Loss 2.3294\n",
            "Epoch 4 Step 67600 Loss 2.1816\n",
            "Epoch 4 Step 67800 Loss 2.5296\n",
            "Epoch 4 Step 68000 Loss 2.5587\n",
            "Epoch 4 Step 68200 Loss 2.4935\n",
            "Epoch 4 Step 68400 Loss 2.6438\n",
            "Epoch 4 Step 68600 Loss 2.5101\n",
            "Epoch 4 Step 68800 Loss 2.3459\n",
            "Epoch 4 Step 69000 Loss 2.6146\n",
            "Epoch 4 Step 69200 Loss 2.7020\n",
            "Epoch 4 Step 69400 Loss 2.1080\n",
            "Epoch 4 Step 69600 Loss 2.5063\n",
            "Epoch 4 Step 69800 Loss 2.3519\n",
            "Epoch 4 Step 70000 Loss 2.4197\n",
            "Epoch 4 Step 70200 Loss 2.4119\n",
            "Epoch 4 Step 70400 Loss 2.7447\n",
            "Epoch 4 Step 70600 Loss 2.4259\n",
            "Epoch 4 Step 70800 Loss 2.3759\n",
            "Epoch 4 Step 71000 Loss 2.4830\n",
            "Epoch 4 Step 71200 Loss 2.3927\n",
            "Epoch 4 Step 71400 Loss 2.4562\n",
            "Epoch 4 Step 71600 Loss 2.2587\n",
            "Epoch 4 Step 71800 Loss 2.4513\n",
            "Epoch 4 Step 72000 Loss 2.6850\n",
            "Epoch 4 Step 72200 Loss 2.5349\n",
            "Epoch 4 Step 72400 Loss 2.3696\n",
            "Epoch 4 Step 72600 Loss 2.2838\n",
            "Epoch 4 Step 72800 Loss 2.4368\n",
            "Epoch 4 Step 73000 Loss 2.8407\n",
            "Epoch 4 Step 73200 Loss 1.9952\n",
            "Epoch 4 Step 73400 Loss 2.1805\n",
            "Epoch 4 Step 73600 Loss 2.4390\n",
            "Epoch 4 Step 73800 Loss 2.4339\n",
            "Epoch 4 Step 74000 Loss 2.4693\n",
            "Epoch 4 Step 74200 Loss 2.3789\n",
            "Epoch 4 Step 74400 Loss 2.2372\n",
            "Epoch 4 Step 74600 Loss 2.3647\n",
            "Epoch 4 Step 74800 Loss 2.5353\n",
            "Epoch 4 Step 75000 Loss 2.3967\n",
            "Epoch 4 Step 75200 Loss 2.2923\n",
            "Epoch 4 Step 75400 Loss 2.1501\n",
            "Epoch 4 Step 75600 Loss 2.5391\n",
            "Epoch 4 Step 75800 Loss 2.5139\n",
            "Epoch 4 Step 76000 Loss 2.3524\n",
            "Epoch 4 Step 76200 Loss 2.3138\n",
            "Epoch 4 Step 76400 Loss 2.5154\n",
            "Epoch 4 Step 76600 Loss 2.2193\n",
            "Epoch 4 Step 76800 Loss 2.1267\n",
            "Epoch 4 Step 77000 Loss 2.3150\n",
            "Epoch 4 Step 77200 Loss 2.6863\n",
            "Epoch 4 Step 77400 Loss 2.0146\n",
            "Epoch 4 Step 77600 Loss 2.3681\n",
            "Epoch 4 Step 77800 Loss 2.3182\n",
            "Epoch 4 Step 78000 Loss 2.1626\n",
            "Epoch 4 Step 78200 Loss 2.2068\n",
            "Epoch 4 Step 78400 Loss 2.6444\n",
            "Epoch 4 Step 78600 Loss 2.2743\n",
            "Epoch 4 Step 78800 Loss 2.4816\n",
            "Epoch 4 Step 79000 Loss 2.4414\n",
            "Epoch 4 Step 79200 Loss 2.6148\n",
            "Epoch 4 Step 79400 Loss 2.3573\n",
            "Epoch 4 Step 79600 Loss 2.4200\n",
            "Epoch 4 Step 79800 Loss 2.3608\n",
            "Epoch 4 Step 80000 Loss 2.1802\n",
            "Epoch 4 Step 80200 Loss 2.3733\n",
            "Epoch 4 Step 80400 Loss 2.3695\n",
            "Epoch 4 Step 80600 Loss 2.5320\n",
            "Epoch 4 Step 80800 Loss 2.4366\n",
            "Epoch 4 Step 81000 Loss 2.2332\n",
            "Epoch 4 Step 81200 Loss 2.5943\n",
            "Epoch 4 Step 81400 Loss 2.8908\n",
            "Epoch 4 Step 81600 Loss 2.5880\n",
            "Epoch 4 Step 81800 Loss 2.3193\n",
            "Epoch 4 Step 82000 Loss 2.3221\n",
            "Epoch 4 Step 82200 Loss 2.6560\n",
            "Epoch 4 Step 82400 Loss 2.3186\n",
            "Epoch 4 Step 82600 Loss 2.5882\n",
            "Epoch 4 Step 82800 Loss 2.6563\n",
            "Epoch 4 Step 83000 Loss 2.1929\n",
            "Epoch 4 Step 83200 Loss 2.4068\n",
            "Epoch 4 Step 83400 Loss 2.2064\n",
            "Epoch 4 Step 83600 Loss 2.5579\n",
            "Epoch 4 Step 83800 Loss 2.1768\n",
            "Epoch 4 Step 84000 Loss 2.6204\n",
            "Epoch 4 Step 84200 Loss 2.7839\n",
            "Epoch 4 Step 84400 Loss 2.6268\n",
            "Epoch 4 Step 84600 Loss 2.5387\n",
            "Epoch 4 Step 84800 Loss 2.3044\n",
            "Epoch 4 Step 85000 Loss 2.4762\n",
            "Epoch 4 Step 85200 Loss 2.5006\n",
            "Epoch 4 Step 85400 Loss 2.4832\n",
            "Epoch 4 Step 85600 Loss 2.2929\n",
            "Epoch 4 Step 85800 Loss 2.1942\n",
            "Epoch 4 Step 86000 Loss 2.7428\n",
            "Epoch 4 Step 86200 Loss 2.5481\n",
            "Epoch 4 Step 86400 Loss 2.2878\n",
            "Epoch 4 Step 86600 Loss 2.4851\n",
            "Epoch 4 Step 86800 Loss 2.7553\n",
            "Epoch 4 Step 87000 Loss 2.2639\n",
            "Epoch 4 Step 87200 Loss 2.3375\n",
            "Epoch 4 Step 87400 Loss 2.2782\n",
            "Epoch 4 Step 87600 Loss 3.0353\n",
            "Epoch 4 Step 87800 Loss 2.4289\n",
            "Epoch 4 Step 88000 Loss 2.3261\n",
            "Epoch 4 Step 88200 Loss 2.4388\n",
            "Epoch 4 Step 88400 Loss 2.1605\n",
            "Epoch 4 Step 88600 Loss 2.2827\n",
            "Epoch 4 Step 88800 Loss 2.2526\n",
            "Epoch 4 Step 89000 Loss 2.4823\n",
            "Epoch 4 Step 89200 Loss 2.3623\n",
            "Epoch 4 Step 89400 Loss 2.2287\n",
            "Epoch 4 Step 89600 Loss 2.3484\n",
            "Epoch 4 Step 89800 Loss 2.4075\n",
            "Epoch 4 Step 90000 Loss 2.4855\n",
            "Epoch 4 Step 90200 Loss 2.1576\n",
            "Epoch 4 Step 90400 Loss 2.4504\n",
            "Epoch 4 Step 90600 Loss 2.5282\n",
            "Epoch 4 Step 90800 Loss 2.2270\n",
            "Epoch 4 Step 91000 Loss 2.3359\n",
            "Epoch 4 Step 91200 Loss 2.1216\n",
            "Epoch 4 Step 91400 Loss 2.1832\n",
            "Epoch 4 Step 91600 Loss 2.6228\n",
            "Epoch 4 finished. Last loss = 2.5095\n",
            "Epoch 5 Step 0 Loss 2.1548\n",
            "Epoch 5 Step 200 Loss 2.3848\n",
            "Epoch 5 Step 400 Loss 2.8296\n",
            "Epoch 5 Step 600 Loss 2.5955\n",
            "Epoch 5 Step 800 Loss 2.3497\n",
            "Epoch 5 Step 1000 Loss 2.5357\n",
            "Epoch 5 Step 1200 Loss 2.3870\n",
            "Epoch 5 Step 1400 Loss 2.3601\n",
            "Epoch 5 Step 1600 Loss 2.0825\n",
            "Epoch 5 Step 1800 Loss 2.0148\n",
            "Epoch 5 Step 2000 Loss 2.3621\n",
            "Epoch 5 Step 2200 Loss 2.5015\n",
            "Epoch 5 Step 2400 Loss 2.2831\n",
            "Epoch 5 Step 2600 Loss 2.1077\n",
            "Epoch 5 Step 2800 Loss 2.4454\n",
            "Epoch 5 Step 3000 Loss 2.2726\n",
            "Epoch 5 Step 3200 Loss 2.4749\n",
            "Epoch 5 Step 3400 Loss 2.4594\n",
            "Epoch 5 Step 3600 Loss 2.5666\n",
            "Epoch 5 Step 3800 Loss 2.4108\n",
            "Epoch 5 Step 4000 Loss 2.2663\n",
            "Epoch 5 Step 4200 Loss 2.3957\n",
            "Epoch 5 Step 4400 Loss 2.4484\n",
            "Epoch 5 Step 4600 Loss 2.3309\n",
            "Epoch 5 Step 4800 Loss 2.5860\n",
            "Epoch 5 Step 5000 Loss 2.0421\n",
            "Epoch 5 Step 5200 Loss 2.4616\n",
            "Epoch 5 Step 5400 Loss 2.6424\n",
            "Epoch 5 Step 5600 Loss 2.6169\n",
            "Epoch 5 Step 5800 Loss 2.3374\n",
            "Epoch 5 Step 6000 Loss 2.4060\n",
            "Epoch 5 Step 6200 Loss 2.6080\n",
            "Epoch 5 Step 6400 Loss 2.3630\n",
            "Epoch 5 Step 6600 Loss 2.3202\n",
            "Epoch 5 Step 6800 Loss 2.5805\n",
            "Epoch 5 Step 7000 Loss 2.3896\n",
            "Epoch 5 Step 7200 Loss 2.3140\n",
            "Epoch 5 Step 7400 Loss 2.0694\n",
            "Epoch 5 Step 7600 Loss 2.6197\n",
            "Epoch 5 Step 7800 Loss 2.4321\n",
            "Epoch 5 Step 8000 Loss 2.3386\n",
            "Epoch 5 Step 8200 Loss 2.6457\n",
            "Epoch 5 Step 8400 Loss 2.6230\n",
            "Epoch 5 Step 8600 Loss 2.4100\n",
            "Epoch 5 Step 8800 Loss 2.3503\n",
            "Epoch 5 Step 9000 Loss 2.2951\n",
            "Epoch 5 Step 9200 Loss 2.4270\n",
            "Epoch 5 Step 9400 Loss 2.2260\n",
            "Epoch 5 Step 9600 Loss 2.3457\n",
            "Epoch 5 Step 9800 Loss 2.7820\n",
            "Epoch 5 Step 10000 Loss 2.5439\n",
            "Epoch 5 Step 10200 Loss 2.6739\n",
            "Epoch 5 Step 10400 Loss 2.0582\n",
            "Epoch 5 Step 10600 Loss 2.3793\n",
            "Epoch 5 Step 10800 Loss 2.4223\n",
            "Epoch 5 Step 11000 Loss 2.4763\n",
            "Epoch 5 Step 11200 Loss 2.4713\n",
            "Epoch 5 Step 11400 Loss 2.0721\n",
            "Epoch 5 Step 11600 Loss 2.4487\n",
            "Epoch 5 Step 11800 Loss 2.2548\n",
            "Epoch 5 Step 12000 Loss 2.2925\n",
            "Epoch 5 Step 12200 Loss 2.4771\n",
            "Epoch 5 Step 12400 Loss 2.4024\n",
            "Epoch 5 Step 12600 Loss 2.3753\n",
            "Epoch 5 Step 12800 Loss 2.5247\n",
            "Epoch 5 Step 13000 Loss 2.3317\n",
            "Epoch 5 Step 13200 Loss 2.1154\n",
            "Epoch 5 Step 13400 Loss 2.3652\n",
            "Epoch 5 Step 13600 Loss 2.4930\n",
            "Epoch 5 Step 13800 Loss 2.4395\n",
            "Epoch 5 Step 14000 Loss 2.3584\n",
            "Epoch 5 Step 14200 Loss 2.3107\n",
            "Epoch 5 Step 14400 Loss 2.4723\n",
            "Epoch 5 Step 14600 Loss 2.4177\n",
            "Epoch 5 Step 14800 Loss 2.3617\n",
            "Epoch 5 Step 15000 Loss 2.1544\n",
            "Epoch 5 Step 15200 Loss 2.3808\n",
            "Epoch 5 Step 15400 Loss 2.3454\n",
            "Epoch 5 Step 15600 Loss 2.5115\n",
            "Epoch 5 Step 15800 Loss 2.4862\n",
            "Epoch 5 Step 16000 Loss 2.4472\n",
            "Epoch 5 Step 16200 Loss 2.1422\n",
            "Epoch 5 Step 16400 Loss 2.1497\n",
            "Epoch 5 Step 16600 Loss 2.3524\n",
            "Epoch 5 Step 16800 Loss 2.3262\n",
            "Epoch 5 Step 17000 Loss 2.3946\n",
            "Epoch 5 Step 17200 Loss 2.2745\n",
            "Epoch 5 Step 17400 Loss 2.5763\n",
            "Epoch 5 Step 17600 Loss 2.4761\n",
            "Epoch 5 Step 17800 Loss 2.1185\n",
            "Epoch 5 Step 18000 Loss 2.5027\n",
            "Epoch 5 Step 18200 Loss 2.2272\n",
            "Epoch 5 Step 18400 Loss 2.6834\n",
            "Epoch 5 Step 18600 Loss 2.4877\n",
            "Epoch 5 Step 18800 Loss 2.2758\n",
            "Epoch 5 Step 19000 Loss 2.2869\n",
            "Epoch 5 Step 19200 Loss 2.6265\n",
            "Epoch 5 Step 19400 Loss 2.4235\n",
            "Epoch 5 Step 19600 Loss 2.3319\n",
            "Epoch 5 Step 19800 Loss 2.6390\n",
            "Epoch 5 Step 20000 Loss 2.1402\n",
            "Epoch 5 Step 20200 Loss 2.3849\n",
            "Epoch 5 Step 20400 Loss 2.6688\n",
            "Epoch 5 Step 20600 Loss 2.1654\n",
            "Epoch 5 Step 20800 Loss 2.4188\n",
            "Epoch 5 Step 21000 Loss 2.4463\n",
            "Epoch 5 Step 21200 Loss 2.3943\n",
            "Epoch 5 Step 21400 Loss 2.1400\n",
            "Epoch 5 Step 21600 Loss 2.2194\n",
            "Epoch 5 Step 21800 Loss 2.3858\n",
            "Epoch 5 Step 22000 Loss 2.7021\n",
            "Epoch 5 Step 22200 Loss 2.5882\n",
            "Epoch 5 Step 22400 Loss 2.4541\n",
            "Epoch 5 Step 22600 Loss 2.3587\n",
            "Epoch 5 Step 22800 Loss 2.3121\n",
            "Epoch 5 Step 23000 Loss 2.3495\n",
            "Epoch 5 Step 23200 Loss 2.3844\n",
            "Epoch 5 Step 23400 Loss 2.5574\n",
            "Epoch 5 Step 23600 Loss 2.2968\n",
            "Epoch 5 Step 23800 Loss 2.1753\n",
            "Epoch 5 Step 24000 Loss 2.2119\n",
            "Epoch 5 Step 24200 Loss 2.2586\n",
            "Epoch 5 Step 24400 Loss 2.5854\n",
            "Epoch 5 Step 24600 Loss 2.1120\n",
            "Epoch 5 Step 24800 Loss 2.1045\n",
            "Epoch 5 Step 25000 Loss 2.5293\n",
            "Epoch 5 Step 25200 Loss 2.4298\n",
            "Epoch 5 Step 25400 Loss 2.2928\n",
            "Epoch 5 Step 25600 Loss 2.5162\n",
            "Epoch 5 Step 25800 Loss 2.4106\n",
            "Epoch 5 Step 26000 Loss 2.5174\n",
            "Epoch 5 Step 26200 Loss 2.6039\n",
            "Epoch 5 Step 26400 Loss 2.1399\n",
            "Epoch 5 Step 26600 Loss 2.5914\n",
            "Epoch 5 Step 26800 Loss 2.3651\n",
            "Epoch 5 Step 27000 Loss 2.3927\n",
            "Epoch 5 Step 27200 Loss 2.1544\n",
            "Epoch 5 Step 27400 Loss 2.2345\n",
            "Epoch 5 Step 27600 Loss 2.3448\n",
            "Epoch 5 Step 27800 Loss 2.2161\n",
            "Epoch 5 Step 28000 Loss 2.4856\n",
            "Epoch 5 Step 28200 Loss 2.2937\n",
            "Epoch 5 Step 28400 Loss 2.0507\n",
            "Epoch 5 Step 28600 Loss 2.5212\n",
            "Epoch 5 Step 28800 Loss 2.4310\n",
            "Epoch 5 Step 29000 Loss 2.3413\n",
            "Epoch 5 Step 29200 Loss 2.3197\n",
            "Epoch 5 Step 29400 Loss 2.3082\n",
            "Epoch 5 Step 29600 Loss 2.3046\n",
            "Epoch 5 Step 29800 Loss 2.4076\n",
            "Epoch 5 Step 30000 Loss 2.3074\n",
            "Epoch 5 Step 30200 Loss 2.2747\n",
            "Epoch 5 Step 30400 Loss 2.4794\n",
            "Epoch 5 Step 30600 Loss 2.3705\n",
            "Epoch 5 Step 30800 Loss 2.3897\n",
            "Epoch 5 Step 31000 Loss 2.1339\n",
            "Epoch 5 Step 31200 Loss 2.2586\n",
            "Epoch 5 Step 31400 Loss 2.2477\n",
            "Epoch 5 Step 31600 Loss 2.4936\n",
            "Epoch 5 Step 31800 Loss 2.6326\n",
            "Epoch 5 Step 32000 Loss 2.3364\n",
            "Epoch 5 Step 32200 Loss 2.2094\n",
            "Epoch 5 Step 32400 Loss 2.4882\n",
            "Epoch 5 Step 32600 Loss 2.3050\n",
            "Epoch 5 Step 32800 Loss 2.3784\n",
            "Epoch 5 Step 33000 Loss 2.3819\n",
            "Epoch 5 Step 33200 Loss 2.1742\n",
            "Epoch 5 Step 33400 Loss 2.2339\n",
            "Epoch 5 Step 33600 Loss 2.2133\n",
            "Epoch 5 Step 33800 Loss 2.4235\n",
            "Epoch 5 Step 34000 Loss 2.1821\n",
            "Epoch 5 Step 34200 Loss 2.2576\n",
            "Epoch 5 Step 34400 Loss 2.4449\n",
            "Epoch 5 Step 34600 Loss 2.3601\n",
            "Epoch 5 Step 34800 Loss 2.3653\n",
            "Epoch 5 Step 35000 Loss 2.2512\n",
            "Epoch 5 Step 35200 Loss 2.1661\n",
            "Epoch 5 Step 35400 Loss 2.3125\n",
            "Epoch 5 Step 35600 Loss 2.2134\n",
            "Epoch 5 Step 35800 Loss 2.5138\n",
            "Epoch 5 Step 36000 Loss 2.3268\n",
            "Epoch 5 Step 36200 Loss 2.6135\n",
            "Epoch 5 Step 36400 Loss 2.0996\n",
            "Epoch 5 Step 36600 Loss 2.4425\n",
            "Epoch 5 Step 36800 Loss 2.4905\n",
            "Epoch 5 Step 37000 Loss 2.3615\n",
            "Epoch 5 Step 37200 Loss 2.4232\n",
            "Epoch 5 Step 37400 Loss 2.3561\n",
            "Epoch 5 Step 37600 Loss 2.4362\n",
            "Epoch 5 Step 37800 Loss 2.2513\n",
            "Epoch 5 Step 38000 Loss 2.3594\n",
            "Epoch 5 Step 38200 Loss 2.6114\n",
            "Epoch 5 Step 38400 Loss 2.4702\n",
            "Epoch 5 Step 38600 Loss 2.3914\n",
            "Epoch 5 Step 38800 Loss 2.4472\n",
            "Epoch 5 Step 39000 Loss 2.2333\n",
            "Epoch 5 Step 39200 Loss 2.3472\n",
            "Epoch 5 Step 39400 Loss 2.2637\n",
            "Epoch 5 Step 39600 Loss 2.3701\n",
            "Epoch 5 Step 39800 Loss 2.3185\n",
            "Epoch 5 Step 40000 Loss 2.2328\n",
            "Epoch 5 Step 40200 Loss 2.4325\n",
            "Epoch 5 Step 40400 Loss 2.5178\n",
            "Epoch 5 Step 40600 Loss 2.3979\n",
            "Epoch 5 Step 40800 Loss 2.5571\n",
            "Epoch 5 Step 41000 Loss 2.1745\n",
            "Epoch 5 Step 41200 Loss 2.2971\n",
            "Epoch 5 Step 41400 Loss 2.3308\n",
            "Epoch 5 Step 41600 Loss 2.0890\n",
            "Epoch 5 Step 41800 Loss 2.4618\n",
            "Epoch 5 Step 42000 Loss 2.4408\n",
            "Epoch 5 Step 42200 Loss 2.5369\n",
            "Epoch 5 Step 42400 Loss 2.4352\n",
            "Epoch 5 Step 42600 Loss 2.3732\n",
            "Epoch 5 Step 42800 Loss 2.2716\n",
            "Epoch 5 Step 43000 Loss 2.1530\n",
            "Epoch 5 Step 43200 Loss 2.3271\n",
            "Epoch 5 Step 43400 Loss 2.4084\n",
            "Epoch 5 Step 43600 Loss 2.1713\n",
            "Epoch 5 Step 43800 Loss 2.3354\n",
            "Epoch 5 Step 44000 Loss 2.5229\n",
            "Epoch 5 Step 44200 Loss 2.2003\n",
            "Epoch 5 Step 44400 Loss 2.2429\n",
            "Epoch 5 Step 44600 Loss 2.4099\n",
            "Epoch 5 Step 44800 Loss 2.1470\n",
            "Epoch 5 Step 45000 Loss 2.5189\n",
            "Epoch 5 Step 45200 Loss 2.2593\n",
            "Epoch 5 Step 45400 Loss 2.1242\n",
            "Epoch 5 Step 45600 Loss 2.5158\n",
            "Epoch 5 Step 45800 Loss 2.2736\n",
            "Epoch 5 Step 46000 Loss 2.4626\n",
            "Epoch 5 Step 46200 Loss 2.0313\n",
            "Epoch 5 Step 46400 Loss 2.3447\n",
            "Epoch 5 Step 46600 Loss 2.0378\n",
            "Epoch 5 Step 46800 Loss 2.2023\n",
            "Epoch 5 Step 47000 Loss 2.4206\n",
            "Epoch 5 Step 47200 Loss 2.8151\n",
            "Epoch 5 Step 47400 Loss 2.3192\n",
            "Epoch 5 Step 47600 Loss 2.2237\n",
            "Epoch 5 Step 47800 Loss 2.1523\n",
            "Epoch 5 Step 48000 Loss 2.3232\n",
            "Epoch 5 Step 48200 Loss 2.4538\n",
            "Epoch 5 Step 48400 Loss 2.4053\n",
            "Epoch 5 Step 48600 Loss 1.9946\n",
            "Epoch 5 Step 48800 Loss 2.0869\n",
            "Epoch 5 Step 49000 Loss 2.6629\n",
            "Epoch 5 Step 49200 Loss 2.3588\n",
            "Epoch 5 Step 49400 Loss 2.2028\n",
            "Epoch 5 Step 49600 Loss 2.2617\n",
            "Epoch 5 Step 49800 Loss 2.3195\n",
            "Epoch 5 Step 50000 Loss 2.0324\n",
            "Epoch 5 Step 50200 Loss 2.2577\n",
            "Epoch 5 Step 50400 Loss 2.1547\n",
            "Epoch 5 Step 50600 Loss 2.3551\n",
            "Epoch 5 Step 50800 Loss 2.2120\n",
            "Epoch 5 Step 51000 Loss 2.4303\n",
            "Epoch 5 Step 51200 Loss 2.2406\n",
            "Epoch 5 Step 51400 Loss 2.3110\n",
            "Epoch 5 Step 51600 Loss 2.5977\n",
            "Epoch 5 Step 51800 Loss 2.3052\n",
            "Epoch 5 Step 52000 Loss 2.2122\n",
            "Epoch 5 Step 52200 Loss 2.2641\n",
            "Epoch 5 Step 52400 Loss 2.3162\n",
            "Epoch 5 Step 52600 Loss 2.3904\n",
            "Epoch 5 Step 52800 Loss 2.4791\n",
            "Epoch 5 Step 53000 Loss 2.4591\n",
            "Epoch 5 Step 53200 Loss 2.5644\n",
            "Epoch 5 Step 53400 Loss 2.2584\n",
            "Epoch 5 Step 53600 Loss 2.4608\n",
            "Epoch 5 Step 53800 Loss 2.4688\n",
            "Epoch 5 Step 54000 Loss 2.4905\n",
            "Epoch 5 Step 54200 Loss 2.2950\n",
            "Epoch 5 Step 54400 Loss 2.3341\n",
            "Epoch 5 Step 54600 Loss 2.1598\n",
            "Epoch 5 Step 54800 Loss 2.3234\n",
            "Epoch 5 Step 55000 Loss 2.3514\n",
            "Epoch 5 Step 55200 Loss 2.2671\n",
            "Epoch 5 Step 55400 Loss 2.1978\n",
            "Epoch 5 Step 55600 Loss 2.1464\n",
            "Epoch 5 Step 55800 Loss 2.4106\n",
            "Epoch 5 Step 56000 Loss 2.2850\n",
            "Epoch 5 Step 56200 Loss 2.6078\n",
            "Epoch 5 Step 56400 Loss 2.4568\n",
            "Epoch 5 Step 56600 Loss 2.0960\n",
            "Epoch 5 Step 56800 Loss 2.0565\n",
            "Epoch 5 Step 57000 Loss 2.0990\n",
            "Epoch 5 Step 57200 Loss 2.3456\n",
            "Epoch 5 Step 57400 Loss 2.1900\n",
            "Epoch 5 Step 57600 Loss 2.6257\n",
            "Epoch 5 Step 57800 Loss 2.4079\n",
            "Epoch 5 Step 58000 Loss 1.8888\n",
            "Epoch 5 Step 58200 Loss 2.1285\n",
            "Epoch 5 Step 58400 Loss 2.2550\n",
            "Epoch 5 Step 58600 Loss 2.1642\n",
            "Epoch 5 Step 58800 Loss 2.6020\n",
            "Epoch 5 Step 59000 Loss 2.5838\n",
            "Epoch 5 Step 59200 Loss 2.4390\n",
            "Epoch 5 Step 59400 Loss 2.5898\n",
            "Epoch 5 Step 59600 Loss 2.1988\n",
            "Epoch 5 Step 59800 Loss 2.4293\n",
            "Epoch 5 Step 60000 Loss 2.5470\n",
            "Epoch 5 Step 60200 Loss 2.2549\n",
            "Epoch 5 Step 60400 Loss 2.5449\n",
            "Epoch 5 Step 60600 Loss 2.4527\n",
            "Epoch 5 Step 60800 Loss 2.4001\n",
            "Epoch 5 Step 61000 Loss 2.3879\n",
            "Epoch 5 Step 61200 Loss 2.4218\n",
            "Epoch 5 Step 61400 Loss 2.2094\n",
            "Epoch 5 Step 61600 Loss 2.3910\n",
            "Epoch 5 Step 61800 Loss 2.1646\n",
            "Epoch 5 Step 62000 Loss 2.2686\n",
            "Epoch 5 Step 62200 Loss 1.9272\n",
            "Epoch 5 Step 62400 Loss 2.6676\n",
            "Epoch 5 Step 62600 Loss 2.6570\n",
            "Epoch 5 Step 62800 Loss 2.2345\n",
            "Epoch 5 Step 63000 Loss 2.2565\n",
            "Epoch 5 Step 63200 Loss 2.3849\n",
            "Epoch 5 Step 63400 Loss 2.3792\n",
            "Epoch 5 Step 63600 Loss 2.1097\n",
            "Epoch 5 Step 63800 Loss 2.4969\n",
            "Epoch 5 Step 64000 Loss 2.2857\n",
            "Epoch 5 Step 64200 Loss 2.4617\n",
            "Epoch 5 Step 64400 Loss 2.6583\n",
            "Epoch 5 Step 64600 Loss 1.8915\n",
            "Epoch 5 Step 64800 Loss 2.3251\n",
            "Epoch 5 Step 65000 Loss 2.1387\n",
            "Epoch 5 Step 65200 Loss 2.2408\n",
            "Epoch 5 Step 65400 Loss 2.5889\n",
            "Epoch 5 Step 65600 Loss 2.6863\n",
            "Epoch 5 Step 65800 Loss 2.4001\n",
            "Epoch 5 Step 66000 Loss 2.5255\n",
            "Epoch 5 Step 66200 Loss 2.1167\n",
            "Epoch 5 Step 66400 Loss 2.1414\n",
            "Epoch 5 Step 66600 Loss 2.2531\n",
            "Epoch 5 Step 66800 Loss 2.1454\n",
            "Epoch 5 Step 67000 Loss 2.3256\n",
            "Epoch 5 Step 67200 Loss 2.4097\n",
            "Epoch 5 Step 67400 Loss 2.3066\n",
            "Epoch 5 Step 67600 Loss 2.4368\n",
            "Epoch 5 Step 67800 Loss 2.1728\n",
            "Epoch 5 Step 68000 Loss 2.2165\n",
            "Epoch 5 Step 68200 Loss 2.3210\n",
            "Epoch 5 Step 68400 Loss 2.2449\n",
            "Epoch 5 Step 68600 Loss 2.6776\n",
            "Epoch 5 Step 68800 Loss 2.2062\n",
            "Epoch 5 Step 69000 Loss 2.3290\n",
            "Epoch 5 Step 69200 Loss 2.0569\n",
            "Epoch 5 Step 69400 Loss 2.1111\n",
            "Epoch 5 Step 69600 Loss 2.2402\n",
            "Epoch 5 Step 69800 Loss 2.2150\n",
            "Epoch 5 Step 70000 Loss 2.2424\n",
            "Epoch 5 Step 70200 Loss 2.4015\n",
            "Epoch 5 Step 70400 Loss 2.2969\n",
            "Epoch 5 Step 70600 Loss 2.2784\n",
            "Epoch 5 Step 70800 Loss 2.3718\n",
            "Epoch 5 Step 71000 Loss 2.3817\n",
            "Epoch 5 Step 71200 Loss 2.1954\n",
            "Epoch 5 Step 71400 Loss 2.4775\n",
            "Epoch 5 Step 71600 Loss 2.0202\n",
            "Epoch 5 Step 71800 Loss 2.3838\n",
            "Epoch 5 Step 72000 Loss 2.2904\n",
            "Epoch 5 Step 72200 Loss 2.3956\n",
            "Epoch 5 Step 72400 Loss 2.3250\n",
            "Epoch 5 Step 72600 Loss 2.3647\n",
            "Epoch 5 Step 72800 Loss 2.2786\n",
            "Epoch 5 Step 73000 Loss 2.2530\n",
            "Epoch 5 Step 73200 Loss 2.3725\n",
            "Epoch 5 Step 73400 Loss 2.3942\n",
            "Epoch 5 Step 73600 Loss 2.3136\n",
            "Epoch 5 Step 73800 Loss 2.3149\n",
            "Epoch 5 Step 74000 Loss 2.2170\n",
            "Epoch 5 Step 74200 Loss 2.2935\n",
            "Epoch 5 Step 74400 Loss 2.4625\n",
            "Epoch 5 Step 74600 Loss 2.4415\n",
            "Epoch 5 Step 74800 Loss 2.3234\n",
            "Epoch 5 Step 75000 Loss 2.1541\n",
            "Epoch 5 Step 75200 Loss 2.2978\n",
            "Epoch 5 Step 75400 Loss 2.3404\n",
            "Epoch 5 Step 75600 Loss 2.2129\n",
            "Epoch 5 Step 75800 Loss 2.3794\n",
            "Epoch 5 Step 76000 Loss 2.3227\n",
            "Epoch 5 Step 76200 Loss 1.7839\n",
            "Epoch 5 Step 76400 Loss 2.1365\n",
            "Epoch 5 Step 76600 Loss 2.3332\n",
            "Epoch 5 Step 76800 Loss 2.1779\n",
            "Epoch 5 Step 77000 Loss 2.1807\n",
            "Epoch 5 Step 77200 Loss 2.3541\n",
            "Epoch 5 Step 77400 Loss 2.3387\n",
            "Epoch 5 Step 77600 Loss 2.2927\n",
            "Epoch 5 Step 77800 Loss 2.4487\n",
            "Epoch 5 Step 78000 Loss 2.3612\n",
            "Epoch 5 Step 78200 Loss 2.3061\n",
            "Epoch 5 Step 78400 Loss 2.0698\n",
            "Epoch 5 Step 78600 Loss 2.3798\n",
            "Epoch 5 Step 78800 Loss 1.9602\n",
            "Epoch 5 Step 79000 Loss 2.3581\n",
            "Epoch 5 Step 79200 Loss 2.3313\n",
            "Epoch 5 Step 79400 Loss 2.1283\n",
            "Epoch 5 Step 79600 Loss 2.5761\n",
            "Epoch 5 Step 79800 Loss 2.5483\n",
            "Epoch 5 Step 80000 Loss 2.6744\n",
            "Epoch 5 Step 80200 Loss 2.2640\n",
            "Epoch 5 Step 80400 Loss 2.4822\n",
            "Epoch 5 Step 80600 Loss 2.2949\n",
            "Epoch 5 Step 80800 Loss 2.4329\n",
            "Epoch 5 Step 81000 Loss 2.7229\n",
            "Epoch 5 Step 81200 Loss 2.5936\n",
            "Epoch 5 Step 81400 Loss 2.1040\n",
            "Epoch 5 Step 81600 Loss 2.3697\n",
            "Epoch 5 Step 81800 Loss 2.3036\n",
            "Epoch 5 Step 82000 Loss 2.2685\n",
            "Epoch 5 Step 82200 Loss 2.2961\n",
            "Epoch 5 Step 82400 Loss 2.3410\n",
            "Epoch 5 Step 82600 Loss 2.4489\n",
            "Epoch 5 Step 82800 Loss 2.3084\n",
            "Epoch 5 Step 83000 Loss 2.4752\n",
            "Epoch 5 Step 83200 Loss 2.3662\n",
            "Epoch 5 Step 83400 Loss 2.3292\n",
            "Epoch 5 Step 83600 Loss 2.5683\n",
            "Epoch 5 Step 83800 Loss 2.3347\n",
            "Epoch 5 Step 84000 Loss 2.2714\n",
            "Epoch 5 Step 84200 Loss 2.4851\n",
            "Epoch 5 Step 84400 Loss 2.2478\n",
            "Epoch 5 Step 84600 Loss 2.1454\n",
            "Epoch 5 Step 84800 Loss 2.4695\n",
            "Epoch 5 Step 85000 Loss 2.3375\n",
            "Epoch 5 Step 85200 Loss 2.2102\n",
            "Epoch 5 Step 85400 Loss 2.3649\n",
            "Epoch 5 Step 85600 Loss 2.5484\n",
            "Epoch 5 Step 85800 Loss 2.4820\n",
            "Epoch 5 Step 86000 Loss 2.2476\n",
            "Epoch 5 Step 86200 Loss 2.0570\n",
            "Epoch 5 Step 86400 Loss 2.4314\n",
            "Epoch 5 Step 86600 Loss 2.1802\n",
            "Epoch 5 Step 86800 Loss 2.4865\n",
            "Epoch 5 Step 87000 Loss 1.9477\n",
            "Epoch 5 Step 87200 Loss 1.8798\n",
            "Epoch 5 Step 87400 Loss 2.2184\n",
            "Epoch 5 Step 87600 Loss 2.3489\n",
            "Epoch 5 Step 87800 Loss 2.4253\n",
            "Epoch 5 Step 88000 Loss 2.2025\n",
            "Epoch 5 Step 88200 Loss 2.3830\n",
            "Epoch 5 Step 88400 Loss 2.1963\n",
            "Epoch 5 Step 88600 Loss 2.2116\n",
            "Epoch 5 Step 88800 Loss 1.8396\n",
            "Epoch 5 Step 89000 Loss 2.5205\n",
            "Epoch 5 Step 89200 Loss 2.3685\n",
            "Epoch 5 Step 89400 Loss 1.8560\n",
            "Epoch 5 Step 89600 Loss 2.6290\n",
            "Epoch 5 Step 89800 Loss 2.2788\n",
            "Epoch 5 Step 90000 Loss 2.3036\n",
            "Epoch 5 Step 90200 Loss 2.3278\n",
            "Epoch 5 Step 90400 Loss 2.3884\n",
            "Epoch 5 Step 90600 Loss 2.2162\n",
            "Epoch 5 Step 90800 Loss 2.2591\n",
            "Epoch 5 Step 91000 Loss 2.3731\n",
            "Epoch 5 Step 91200 Loss 2.2556\n",
            "Epoch 5 Step 91400 Loss 2.4484\n",
            "Epoch 5 Step 91600 Loss 2.4873\n",
            "Epoch 5 finished. Last loss = 2.2095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"trained_model.pth\")\n",
        "from google.colab import files\n",
        "files.download(\"trained_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0aQGGKDfWQlG",
        "outputId": "ec32f48a-82b4-441e-f420-585ec088cde7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2399d7d9-0b7d-404b-a43e-e3409feff205\", \"trained_model.pth\", 28225938)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model)\n",
        "\n",
        "print(\"\\nParameter details:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {param.shape}, requires_grad={param.requires_grad}\")\n",
        "\n",
        "print(\"\\nModel config:\")\n",
        "print(model.config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1IIx9R3l53n",
        "outputId": "4830da6b-d5a3-4b8f-a94c-d6bb8557d1de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 7,049,216\n",
            "Trainable parameters: 7,049,216\n",
            "\n",
            "Model architecture:\n",
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(8000, 256)\n",
            "    (wpe): Embedding(1024, 256)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=768, nx=256)\n",
            "          (c_proj): Conv1D(nf=256, nx=256)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=1024, nx=256)\n",
            "          (c_proj): Conv1D(nf=256, nx=1024)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=256, out_features=8000, bias=False)\n",
            ")\n",
            "\n",
            "Parameter details:\n",
            "transformer.wte.weight: torch.Size([8000, 256]), requires_grad=True\n",
            "transformer.wpe.weight: torch.Size([1024, 256]), requires_grad=True\n",
            "transformer.h.0.ln_1.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.0.ln_1.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.0.attn.c_attn.weight: torch.Size([256, 768]), requires_grad=True\n",
            "transformer.h.0.attn.c_attn.bias: torch.Size([768]), requires_grad=True\n",
            "transformer.h.0.attn.c_proj.weight: torch.Size([256, 256]), requires_grad=True\n",
            "transformer.h.0.attn.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.0.ln_2.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.0.ln_2.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.0.mlp.c_fc.weight: torch.Size([256, 1024]), requires_grad=True\n",
            "transformer.h.0.mlp.c_fc.bias: torch.Size([1024]), requires_grad=True\n",
            "transformer.h.0.mlp.c_proj.weight: torch.Size([1024, 256]), requires_grad=True\n",
            "transformer.h.0.mlp.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.1.ln_1.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.1.ln_1.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.1.attn.c_attn.weight: torch.Size([256, 768]), requires_grad=True\n",
            "transformer.h.1.attn.c_attn.bias: torch.Size([768]), requires_grad=True\n",
            "transformer.h.1.attn.c_proj.weight: torch.Size([256, 256]), requires_grad=True\n",
            "transformer.h.1.attn.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.1.ln_2.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.1.ln_2.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.1.mlp.c_fc.weight: torch.Size([256, 1024]), requires_grad=True\n",
            "transformer.h.1.mlp.c_fc.bias: torch.Size([1024]), requires_grad=True\n",
            "transformer.h.1.mlp.c_proj.weight: torch.Size([1024, 256]), requires_grad=True\n",
            "transformer.h.1.mlp.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.2.ln_1.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.2.ln_1.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.2.attn.c_attn.weight: torch.Size([256, 768]), requires_grad=True\n",
            "transformer.h.2.attn.c_attn.bias: torch.Size([768]), requires_grad=True\n",
            "transformer.h.2.attn.c_proj.weight: torch.Size([256, 256]), requires_grad=True\n",
            "transformer.h.2.attn.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.2.ln_2.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.2.ln_2.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.2.mlp.c_fc.weight: torch.Size([256, 1024]), requires_grad=True\n",
            "transformer.h.2.mlp.c_fc.bias: torch.Size([1024]), requires_grad=True\n",
            "transformer.h.2.mlp.c_proj.weight: torch.Size([1024, 256]), requires_grad=True\n",
            "transformer.h.2.mlp.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.3.ln_1.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.3.ln_1.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.3.attn.c_attn.weight: torch.Size([256, 768]), requires_grad=True\n",
            "transformer.h.3.attn.c_attn.bias: torch.Size([768]), requires_grad=True\n",
            "transformer.h.3.attn.c_proj.weight: torch.Size([256, 256]), requires_grad=True\n",
            "transformer.h.3.attn.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.3.ln_2.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.3.ln_2.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.3.mlp.c_fc.weight: torch.Size([256, 1024]), requires_grad=True\n",
            "transformer.h.3.mlp.c_fc.bias: torch.Size([1024]), requires_grad=True\n",
            "transformer.h.3.mlp.c_proj.weight: torch.Size([1024, 256]), requires_grad=True\n",
            "transformer.h.3.mlp.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.4.ln_1.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.4.ln_1.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.4.attn.c_attn.weight: torch.Size([256, 768]), requires_grad=True\n",
            "transformer.h.4.attn.c_attn.bias: torch.Size([768]), requires_grad=True\n",
            "transformer.h.4.attn.c_proj.weight: torch.Size([256, 256]), requires_grad=True\n",
            "transformer.h.4.attn.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.4.ln_2.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.4.ln_2.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.4.mlp.c_fc.weight: torch.Size([256, 1024]), requires_grad=True\n",
            "transformer.h.4.mlp.c_fc.bias: torch.Size([1024]), requires_grad=True\n",
            "transformer.h.4.mlp.c_proj.weight: torch.Size([1024, 256]), requires_grad=True\n",
            "transformer.h.4.mlp.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.5.ln_1.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.5.ln_1.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.5.attn.c_attn.weight: torch.Size([256, 768]), requires_grad=True\n",
            "transformer.h.5.attn.c_attn.bias: torch.Size([768]), requires_grad=True\n",
            "transformer.h.5.attn.c_proj.weight: torch.Size([256, 256]), requires_grad=True\n",
            "transformer.h.5.attn.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.5.ln_2.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.h.5.ln_2.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.h.5.mlp.c_fc.weight: torch.Size([256, 1024]), requires_grad=True\n",
            "transformer.h.5.mlp.c_fc.bias: torch.Size([1024]), requires_grad=True\n",
            "transformer.h.5.mlp.c_proj.weight: torch.Size([1024, 256]), requires_grad=True\n",
            "transformer.h.5.mlp.c_proj.bias: torch.Size([256]), requires_grad=True\n",
            "transformer.ln_f.weight: torch.Size([256]), requires_grad=True\n",
            "transformer.ln_f.bias: torch.Size([256]), requires_grad=True\n",
            "\n",
            "Model config:\n",
            "GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 256,\n",
            "  \"n_head\": 4,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.54.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 8000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}